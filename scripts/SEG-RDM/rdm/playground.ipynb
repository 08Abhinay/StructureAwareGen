{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d5043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/scratch/gilbreth/abelde/Thesis/StructureAwareGen/scripts/SEG-RDM')\n",
    "\n",
    "sys.argv = [\n",
    "    'rdm.main_rdm',\n",
    "    '--config', 'rdm/configs/rdm_default.yaml',\n",
    "    '--batch_size', '8',\n",
    "    '--input_size', '256',\n",
    "    '--epochs', '1',\n",
    "    '--blr', '1e-6',\n",
    "    '--weight_decay', '0.01',\n",
    "    '--output_dir', '/tmp/seg_rdm_out',\n",
    "    '--data_path', '/path/to/imagenet_or_dummy',\n",
    "]\n",
    "\n",
    "from rdm.main_rdm import get_args_parser, main\n",
    "args = get_args_parser().parse_args()\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbfaed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b10741cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 17:28:36.800749: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769120916.945701 1731195 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769120916.985704 1731195 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1769120917.305921 1731195 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769120917.305947 1731195 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769120917.305950 1731195 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769120917.305952 1731195 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-22 17:28:37.338031: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/abelde/.local/lib/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| distributed init (rank 0): env://, gpu 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W122 17:30:00.709561140 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:30:00.884151] job dir: /scratch/gilbreth/abelde/Thesis/StructureAwareGen/scripts/SEG-RDM/rdm\n",
      "[17:30:00.884323] Namespace(batch_size=8,\n",
      "epochs=200,\n",
      "accum_iter=1,\n",
      "input_size=256,\n",
      "config='/scratch/gilbreth/abelde/Thesis/StructureAwareGen/scripts/SEG-RDM/rdm/configs/rdm_default.yaml',\n",
      "weight_decay=0.01,\n",
      "lr=None,\n",
      "blr=1e-06,\n",
      "min_lr=0.0,\n",
      "cosine_lr=False,\n",
      "warmup_epochs=0,\n",
      "data_path='/scratch/gilbreth/abelde/Thesis/StructureAwareGen/dataset/imagenet-1K-hf/',\n",
      "output_dir='/scratch/gilbreth/abelde/Thesis/StructureAwareGen/scripts/SEG-RDM/rdm/rdm_debug_out',\n",
      "log_dir='./output_dir',\n",
      "device='cuda',\n",
      "seed=0,\n",
      "resume='',\n",
      "start_epoch=0,\n",
      "num_workers=10,\n",
      "pin_mem=True,\n",
      "world_size=1,\n",
      "local_rank=-1,\n",
      "dist_on_itp=False,\n",
      "dist_url='env://',\n",
      "rank=0,\n",
      "gpu=0,\n",
      "distributed=True,\n",
      "dist_backend='nccl')\n",
      "[17:30:38.745684] Dataset ImageFolder\n",
      "    Number of datapoints: 1281167\n",
      "    Root location: /scratch/gilbreth/abelde/Thesis/StructureAwareGen/dataset/imagenet-1K-hf/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)\n",
      "               RandomCrop(size=(256, 256), padding=None)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "           )\n",
      "[17:30:38.746298] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x150c925020a0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abelde/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/home/abelde/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/abelde/.local/lib/python3.9/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/scratch/gilbreth/abelde/Thesis/StructureAwareGen/scripts/SEG-RDM/rdm/pretrained_enc/deit/vits.py:67: UserWarning: Overwriting deit_tiny_patch16_224 in registry with rdm.pretrained_enc.deit.vits.deit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit_tiny_patch16_224(pretrained=False, **kwargs):\n",
      "/scratch/gilbreth/abelde/Thesis/StructureAwareGen/scripts/SEG-RDM/rdm/pretrained_enc/deit/vits.py:82: UserWarning: Overwriting deit_small_patch16_224 in registry with rdm.pretrained_enc.deit.vits.deit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit_small_patch16_224(pretrained=False, **kwargs):\n",
      "/scratch/gilbreth/abelde/Thesis/StructureAwareGen/scripts/SEG-RDM/rdm/pretrained_enc/deit/vits.py:97: UserWarning: Overwriting deit_base_patch16_224 in registry with rdm.pretrained_enc.deit.vits.deit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit_base_patch16_224(pretrained=False, **kwargs):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:30:50.492297] RDM: Running in x0-prediction mode\n",
      "[17:30:50.964015] DiffusionWrapper has 62.72 M params.\n",
      "[17:30:51.007209] Keeping EMAs of 132.\n",
      "[17:30:58.063884] Model = RDM(\n",
      "  (model): DiffusionWrapper(\n",
      "    (diffusion_model): SimpleMLP(\n",
      "      (time_embed): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=256, bias=True)\n",
      "        (1): SiLU()\n",
      "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (input_proj): Linear(in_features=256, out_features=1536, bias=True)\n",
      "      (res_blocks): ModuleList(\n",
      "        (0-11): 12 x ResBlock(\n",
      "          (in_layers): Sequential(\n",
      "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): SiLU()\n",
      "            (2): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          )\n",
      "          (emb_layers): Sequential(\n",
      "            (0): SiLU()\n",
      "            (1): Linear(in_features=256, out_features=1536, bias=True)\n",
      "          )\n",
      "          (out_layers): Sequential(\n",
      "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): SiLU()\n",
      "            (2): Dropout(p=0, inplace=False)\n",
      "            (3): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (out): Sequential(\n",
      "        (0): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Linear(in_features=1536, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (model_ema): LitEma()\n",
      "  (pretrained_encoder): VisionTransformerMoCo(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (patch_drop): Identity()\n",
      "    (norm_pre): Identity()\n",
      "    (blocks): Sequential(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (4): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (5): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (6): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (7): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (8): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (9): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (10): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (11): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (fc_norm): Identity()\n",
      "    (head_drop): Dropout(p=0.0, inplace=False)\n",
      "    (head): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=4096, bias=False)\n",
      "      (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (4): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Linear(in_features=4096, out_features=256, bias=False)\n",
      "      (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (cond_stage_model): ClassEmbedder(\n",
      "    (embedding): Embedding(1, 512)\n",
      "  )\n",
      ")\n",
      "[17:30:58.064193] base lr: 1.00e-06\n",
      "[17:30:58.064215] actual lr: 8.00e-06\n",
      "[17:30:58.064232] accumulate grad iterations: 1\n",
      "[17:30:58.064249] effective batch size: 8\n",
      "[17:30:58.096800] Number of trainable parameters: 62.721792M\n",
      "[17:30:58.097748] AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: True\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 8e-06\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "[17:30:58.099024] Start training for 200 epochs\n",
      "[17:30:58.100638] log_dir: ./output_dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gilbreth/abelde/Thesis/StructureAwareGen/scripts/SEG-RDM/rdm/util.py:427: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self._scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
      "/home/abelde/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 5, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:31:00.982216] Epoch: [0]  [     0/160145]  eta: 5 days, 8:07:37  lr: 0.000008  loss: 0.9961 (0.9961)  time: 2.8803  data: 1.6876  max mem: 2147\n",
      "[17:31:02.538511] Epoch: [0]  [    20/160145]  eta: 9:23:41  lr: 0.000008  loss: 0.9958 (0.9957)  time: 0.0778  data: 0.0033  max mem: 2147\n",
      "[17:31:04.894556] Epoch: [0]  [    40/160145]  eta: 7:21:57  lr: 0.000008  loss: 0.9950 (0.9953)  time: 0.1178  data: 0.0469  max mem: 2147\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m args\u001b[38;5;241m.\u001b[39mgpu \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m          \u001b[38;5;66;03m# or set to your intended GPU index\u001b[39;00m\n\u001b[1;32m     90\u001b[0m args\u001b[38;5;241m.\u001b[39mdist_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv://\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 91\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/gilbreth/abelde/Thesis/StructureAwareGen/scripts/SEG-RDM/rdm/main_rdm.py:207\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mdistributed:\n\u001b[1;32m    205\u001b[0m     data_loader_train\u001b[38;5;241m.\u001b[39msampler\u001b[38;5;241m.\u001b[39mset_epoch(epoch)\n\u001b[0;32m--> 207\u001b[0m train_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_scaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_writer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39moutput_dir \u001b[38;5;129;01mand\u001b[39;00m (epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m25\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m args\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m    214\u001b[0m     util\u001b[38;5;241m.\u001b[39msave_model(\n\u001b[1;32m    215\u001b[0m         args\u001b[38;5;241m=\u001b[39margs, model\u001b[38;5;241m=\u001b[39mmodel, model_without_ddp\u001b[38;5;241m=\u001b[39mmodel_without_ddp, optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m    216\u001b[0m         loss_scaler\u001b[38;5;241m=\u001b[39mloss_scaler, epoch\u001b[38;5;241m=\u001b[39mepoch)\n",
      "File \u001b[0;32m/scratch/gilbreth/abelde/Thesis/StructureAwareGen/scripts/SEG-RDM/rdm/engine_rdm.py:52\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, data_loader, optimizer, device, epoch, loss_scaler, log_writer, args)\u001b[0m\n\u001b[1;32m     49\u001b[0m     sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     51\u001b[0m loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m accum_iter\n\u001b[0;32m---> 52\u001b[0m \u001b[43mloss_scaler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_iter_step\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maccum_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (data_iter_step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m accum_iter \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     59\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/scratch/gilbreth/abelde/Thesis/StructureAwareGen/scripts/SEG-RDM/rdm/util.py:431\u001b[0m, in \u001b[0;36mNativeScalerWithGradNormCount.__call__\u001b[0;34m(self, loss, optimizer, clip_grad, parameters, create_graph, update_grad)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss, optimizer, clip_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    430\u001b[0m              create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, update_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_grad:\n\u001b[1;32m    433\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m clip_grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# -----------------------------\n",
    "# Paths (edit if needed)\n",
    "# -----------------------------\n",
    "repo_root = \"/scratch/gilbreth/abelde/Thesis/StructureAwareGen/scripts/SEG-RDM/\"\n",
    "config_path = \"/scratch/gilbreth/abelde/Thesis/StructureAwareGen/scripts/SEG-RDM/rdm/configs/rdm_default.yaml\"  # relative to repo_root is fine\n",
    "output_dir = os.environ.get(\"OUTPUT_DIR\", \"/scratch/gilbreth/abelde/Thesis/StructureAwareGen/scripts/SEG-RDM/rdm/rdm_debug_out\")\n",
    "imagenet_dir = os.environ.get(\"IMAGENET_DIR\", \"/scratch/gilbreth/abelde/Thesis/StructureAwareGen/dataset/imagenet-1K-hf/\")\n",
    "\n",
    "import os\n",
    "\n",
    "# --- Make env:// rendezvous happy (single process) ---\n",
    "os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Put repo on import path\n",
    "# -----------------------------\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.insert(0, repo_root)\n",
    "\n",
    "\n",
    "\n",
    "# If you already imported these in the notebook, reload to pick up edits/breakpoints\n",
    "import main_rdm\n",
    "importlib.reload(main_rdm)\n",
    "\n",
    "# -----------------------------\n",
    "# Debug tip:\n",
    "# For notebook debugging, DON'T use torch.distributed.launch.\n",
    "# Run single GPU / single process. That means:\n",
    "#   - remove/ignore --dist_url / --nnodes / --node_rank / --nproc_per_node\n",
    "# -----------------------------\n",
    "# sys.argv = [\n",
    "#     \"main_rdm.py\",\n",
    "#     \"--config\", config_path,\n",
    "#     \"--batch_size\", \"128\",\n",
    "#     \"--input_size\", \"256\",\n",
    "#     \"--epochs\", \"200\",\n",
    "#     \"--blr\", \"1e-6\",\n",
    "#     \"--weight_decay\", \"0.01\",\n",
    "#     \"--output_dir\", output_dir,\n",
    "#     \"--data_path\", imagenet_dir,\n",
    "\n",
    "#     # Optional: make sure it doesn't try multi-process\n",
    "#     # (one of these usually exists; keep only the ones your argparse supports)\n",
    "#     \"--world_size\", \"1\",\n",
    "#     \"--local_rank\", \"0\",\n",
    "# ]\n",
    "\n",
    "# # -----------------------------\n",
    "# # Run\n",
    "# # -----------------------------\n",
    "# # Many repos have main() defined; if not, they run under `if __name__ == \"__main__\":`\n",
    "# # We'll try main() first.\n",
    "# if hasattr(main_rdm, \"main\"):\n",
    "#     main_rdm.main()\n",
    "# else:\n",
    "#     # fallback: execute the file as a script\n",
    "#     import runpy\n",
    "#     runpy.run_path(os.path.join(repo_root, \"main_rdm.py\"), run_name=\"__main__\")\n",
    "sys.argv = [\n",
    "    'rdm.main_rdm',\n",
    "    '--config', config_path,\n",
    "    '--batch_size', '8',\n",
    "    '--input_size', '256',\n",
    "    '--epochs', '200',\n",
    "    '--blr', '1e-6',\n",
    "    '--weight_decay', '0.01',\n",
    "    '--output_dir', output_dir,\n",
    "    '--data_path', imagenet_dir,\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from rdm.main_rdm import get_args_parser, main\n",
    "args = get_args_parser().parse_args()\n",
    "# --- Make sure your args reflect single-process ---\n",
    "args.world_size = 1\n",
    "args.rank = 0\n",
    "args.gpu = 0          # or set to your intended GPU index\n",
    "args.dist_url = \"env://\"\n",
    "main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47d00053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13364.48s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:36:58.206180] [21:36:58.206171] [21:36:58.206388] [21:36:58.206162] [21:36:58.206430] [21:36:58.206423] [21:36:58.206462] [21:36:58.206150] [21:36:58.206524] [21:36:58.206516] [21:36:58.206558] [21:36:58.206496] [21:36:58.206596] [21:36:58.206590] [21:36:58.206626] [21:36:58.206134] [21:36:58.206682] [21:36:58.206676] [21:36:58.206713] [21:36:58.206670] [21:36:58.206750] [21:36:58.206744] [21:36:58.206780] [21:36:58.206664] [21:36:58.206823] [21:36:58.206817] [21:36:58.206859] [21:36:58.206811] [21:36:58.206896] [21:36:58.206890] [21:36:58.206927] [21:36:58.206099] [21:36:58.206982] [21:36:58.206977] [21:36:58.207015] [21:36:58.206971] [21:36:58.207052] [21:36:58.207046] [21:36:58.207079] [21:36:58.206965] [21:36:58.207114] [21:36:58.207109] [21:36:58.207148] [21:36:58.207104] [21:36:58.207179] [21:36:58.207174] [21:36:58.207214] [21:36:58.206959] [21:36:58.207279] [21:36:58.207273] [21:36:58.207304] [21:36:58.207265] [21:36:58.207333] [21:36:58.207328] [21:36:58.207358] [21:36:58.207257] [21:36:58.207392] [21:36:58.207388] [21:36:58.207416] [21:36:58.207383] [21:36:58.207445] [21:36:58.207440] [21:36:58.207469] [21:36:58.209550] [21:36:58.209544] [21:36:58.209617] [21:36:58.209539] [21:36:58.209650] [21:36:58.209645] [21:36:58.209674] [21:36:58.209534] [21:36:58.209709] [21:36:58.209704] [21:36:58.209733] [21:36:58.209699] [21:36:58.209762] [21:36:58.209757] [21:36:58.209786] [21:36:58.209529] [21:36:58.209824] [21:36:58.209820] [21:36:58.209848] [21:36:58.209815] [21:36:58.209877] [21:36:58.209873] [21:36:58.209902] [21:36:58.209810] [21:36:58.209935] [21:36:58.209931] [21:36:58.209961] [21:36:58.209926] [21:36:58.209990] [21:36:58.209985] [21:36:58.210015] [21:36:58.209519] [21:36:58.210057] [21:36:58.210053] [21:36:58.210081] [21:36:58.210048] [21:36:58.210109] [21:36:58.210105] [21:36:58.210133] [21:36:58.210044] [21:36:58.210166] [21:36:58.210161] [21:36:58.210190] [21:36:58.210157] [21:36:58.210218] [21:36:58.210214] [21:36:58.210242] [21:36:58.210039] [21:36:58.210279] [21:36:58.210275] [21:36:58.210304] [21:36:58.210271] [21:36:58.210332] [21:36:58.210328] [21:36:58.210356] [21:36:58.210266] [21:36:58.210390] [21:36:58.210385] [21:36:58.210413] [21:36:58.210380] [21:36:58.210442] [21:36:58.210437] [21:36:58.210466] /scratch/gilbreth/abelde/Thesis/StructureAwareGen/scripts/SEG-RDM/rdm\n"
     ]
    }
   ],
   "source": [
    "import runpy\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9aeea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:40:33.266302] [21:40:33.266296] [21:40:33.335361] [21:40:33.266290] [21:40:33.335413] [21:40:33.335406] [21:40:33.335445] [21:40:33.266280] [21:40:33.335488] [21:40:33.335482] [21:40:33.335535] [21:40:33.335476] [21:40:33.335571] [21:40:33.335565] [21:40:33.335600] [21:40:33.266259] [21:40:33.335648] [21:40:33.335642] [21:40:33.335677] [21:40:33.335637] [21:40:33.335712] [21:40:33.335706] [21:40:33.335740] [21:40:33.335631] [21:40:33.335781] [21:40:33.335775] [21:40:33.335810] [21:40:33.335770] [21:40:33.335845] [21:40:33.335839] [21:40:33.335874] [21:40:33.230351] [21:40:33.335926] [21:40:33.335920] [21:40:33.335955] [21:40:33.335915] [21:40:33.335990] [21:40:33.335985] [21:40:33.336019] [21:40:33.335909] [21:40:33.336059] [21:40:33.336054] [21:40:33.336089] [21:40:33.336048] [21:40:33.336119] [21:40:33.336114] [21:40:33.336142] [21:40:33.335904] [21:40:33.336181] [21:40:33.336176] [21:40:33.336204] [21:40:33.336172] [21:40:33.336234] [21:40:33.336229] [21:40:33.336259] [21:40:33.336166] [21:40:33.336291] [21:40:33.336287] [21:40:33.336315] [21:40:33.336282] [21:40:33.336342] [21:40:33.336338] [21:40:33.336366] <class 'dict'>\n",
      "[21:40:33.358656] [21:40:33.358647] [21:40:33.358725] [21:40:33.358638] [21:40:33.358788] [21:40:33.358777] [21:40:33.358838] [21:40:33.358628] [21:40:33.358904] [21:40:33.358895] [21:40:33.358955] [21:40:33.358886] [21:40:33.359017] [21:40:33.359007] [21:40:33.359064] [21:40:33.358618] [21:40:33.359147] [21:40:33.359137] [21:40:33.359176] [21:40:33.359127] [21:40:33.359205] [21:40:33.359200] [21:40:33.359228] [21:40:33.359116] [21:40:33.359261] [21:40:33.359256] [21:40:33.359284] [21:40:33.359252] [21:40:33.359312] [21:40:33.359308] [21:40:33.359336] [21:40:33.358596] [21:40:33.359377] [21:40:33.359373] [21:40:33.359401] [21:40:33.359368] [21:40:33.359428] [21:40:33.359424] [21:40:33.359451] [21:40:33.359364] [21:40:33.359484] [21:40:33.359479] [21:40:33.359516] [21:40:33.359475] [21:40:33.359547] [21:40:33.359542] [21:40:33.359570] [21:40:33.359359] [21:40:33.359607] [21:40:33.359603] [21:40:33.359631] [21:40:33.359598] [21:40:33.359665] [21:40:33.359660] [21:40:33.359689] [21:40:33.359594] [21:40:33.359722] [21:40:33.359717] [21:40:33.359745] [21:40:33.359713] [21:40:33.359773] [21:40:33.359769] [21:40:33.359796] ['model']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "ckpt = torch.load(\"pretrained_enc_ckpts/mocov3/rdm-mocov3vitb.pth\", map_location=\"cpu\")\n",
    "print(type(ckpt))\n",
    "print(list(ckpt.keys()) if isinstance(ckpt, dict) else \"not a dict\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1a6ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:43:59.063084] [21:43:59.063078] [21:43:59.115390] [21:43:59.063073] [21:43:59.115431] [21:43:59.115424] [21:43:59.115456] [21:43:59.063065] [21:43:59.115490] [21:43:59.115485] [21:43:59.115524] [21:43:59.115480] [21:43:59.115555] [21:43:59.115550] [21:43:59.115578] [21:43:59.063054] [21:43:59.115627] [21:43:59.115623] [21:43:59.115653] [21:43:59.115618] [21:43:59.115682] [21:43:59.115677] [21:43:59.115706] [21:43:59.115613] [21:43:59.115738] [21:43:59.115734] [21:43:59.115762] [21:43:59.115729] [21:43:59.115790] [21:43:59.115785] [21:43:59.115814] [21:43:59.063042] [21:43:59.115856] [21:43:59.115852] [21:43:59.115881] [21:43:59.115847] [21:43:59.115909] [21:43:59.115905] [21:43:59.115933] [21:43:59.115843] [21:43:59.115966] [21:43:59.115962] [21:43:59.115991] [21:43:59.115957] [21:43:59.116022] [21:43:59.116017] [21:43:59.116046] [21:43:59.115838] [21:43:59.116083] [21:43:59.116079] [21:43:59.116107] [21:43:59.116074] [21:43:59.116135] [21:43:59.116131] [21:43:59.116159] [21:43:59.116070] [21:43:59.116191] [21:43:59.116187] [21:43:59.116215] [21:43:59.116183] [21:43:59.116244] [21:43:59.116239] [21:43:59.116267] [21:43:59.036055] [21:43:59.116315] [21:43:59.116311] [21:43:59.116340] [21:43:59.116307] [21:43:59.116368] [21:43:59.116363] [21:43:59.116391] [21:43:59.116302] [21:43:59.116425] [21:43:59.116421] [21:43:59.116448] [21:43:59.116416] [21:43:59.116476] [21:43:59.116471] [21:43:59.116508] [21:43:59.116298] [21:43:59.116546] [21:43:59.116541] [21:43:59.116569] [21:43:59.116537] [21:43:59.116597] [21:43:59.116593] [21:43:59.116620] [21:43:59.116532] [21:43:59.116653] [21:43:59.116648] [21:43:59.116676] [21:43:59.116644] [21:43:59.116704] [21:43:59.116699] [21:43:59.116728] [21:43:59.116293] [21:43:59.116770] [21:43:59.116766] [21:43:59.116794] [21:43:59.116761] [21:43:59.116822] [21:43:59.116817] [21:43:59.116846] [21:43:59.116757] [21:43:59.116878] [21:43:59.116874] [21:43:59.116902] [21:43:59.116869] [21:43:59.116930] [21:43:59.116925] [21:43:59.116953] [21:43:59.116752] [21:43:59.116990] [21:43:59.116986] [21:43:59.117014] [21:43:59.116981] [21:43:59.117042] [21:43:59.117037] [21:43:59.117066] [21:43:59.116977] [21:43:59.117098] [21:43:59.117093] [21:43:59.117121] [21:43:59.117089] [21:43:59.117149] [21:43:59.117144] [21:43:59.117172] ckpt keys: ['model']\n",
      "[21:43:59.159886] [21:43:59.159879] [21:43:59.159925] [21:43:59.159874] [21:43:59.159956] [21:43:59.159951] [21:43:59.159981] [21:43:59.159869] [21:43:59.160014] [21:43:59.160010] [21:43:59.160039] [21:43:59.160005] [21:43:59.160067] [21:43:59.160063] [21:43:59.160091] [21:43:59.159864] [21:43:59.160144] [21:43:59.160139] [21:43:59.160170] [21:43:59.160135] [21:43:59.160199] [21:43:59.160195] [21:43:59.160224] [21:43:59.160127] [21:43:59.160257] [21:43:59.160252] [21:43:59.160281] [21:43:59.160248] [21:43:59.160309] [21:43:59.160304] [21:43:59.160332] [21:43:59.159858] [21:43:59.160375] [21:43:59.160370] [21:43:59.160400] [21:43:59.160366] [21:43:59.160428] [21:43:59.160424] [21:43:59.160452] [21:43:59.160361] [21:43:59.160485] [21:43:59.160480] [21:43:59.160520] [21:43:59.160476] [21:43:59.160551] [21:43:59.160546] [21:43:59.160575] [21:43:59.160357] [21:43:59.160613] [21:43:59.160608] [21:43:59.160636] [21:43:59.160604] [21:43:59.160664] [21:43:59.160660] [21:43:59.160688] [21:43:59.160599] [21:43:59.160721] [21:43:59.160716] [21:43:59.160745] [21:43:59.160712] [21:43:59.160773] [21:43:59.160769] [21:43:59.160797] [21:43:59.159847] [21:43:59.160843] [21:43:59.160839] [21:43:59.160867] [21:43:59.160834] [21:43:59.160896] [21:43:59.160891] [21:43:59.160920] [21:43:59.160830] [21:43:59.160953] [21:43:59.160949] [21:43:59.160977] [21:43:59.160944] [21:43:59.161005] [21:43:59.161000] [21:43:59.161028] [21:43:59.160826] [21:43:59.161066] [21:43:59.161061] [21:43:59.161090] [21:43:59.161057] [21:43:59.161118] [21:43:59.161113] [21:43:59.161141] [21:43:59.161052] [21:43:59.161174] [21:43:59.161170] [21:43:59.161198] [21:43:59.161165] [21:43:59.161226] [21:43:59.161221] [21:43:59.161251] [21:43:59.160821] [21:43:59.161293] [21:43:59.161288] [21:43:59.161317] [21:43:59.161284] [21:43:59.161346] [21:43:59.161341] [21:43:59.161370] [21:43:59.161279] [21:43:59.161403] [21:43:59.161398] [21:43:59.161427] [21:43:59.161394] [21:43:59.161456] [21:43:59.161451] [21:43:59.161480] [21:43:59.161275] [21:43:59.161522] [21:43:59.161518] [21:43:59.161546] [21:43:59.161514] [21:43:59.161574] [21:43:59.161570] [21:43:59.161598] [21:43:59.161509] [21:43:59.161631] [21:43:59.161626] [21:43:59.161654] [21:43:59.161622] [21:43:59.161683] [21:43:59.161678] [21:43:59.161706] state keys: 275\n",
      "[21:43:59.161831] [21:43:59.161827] [21:43:59.161860] [21:43:59.161822] [21:43:59.161890] [21:43:59.161885] [21:43:59.161914] [21:43:59.161818] [21:43:59.161948] [21:43:59.161943] [21:43:59.161972] [21:43:59.161939] [21:43:59.162000] [21:43:59.161996] [21:43:59.162024] [21:43:59.161813] [21:43:59.162062] [21:43:59.162057] [21:43:59.162086] [21:43:59.162053] [21:43:59.162115] [21:43:59.162111] [21:43:59.162139] [21:43:59.162048] [21:43:59.162172] [21:43:59.162167] [21:43:59.162197] [21:43:59.162163] [21:43:59.162225] [21:43:59.162221] [21:43:59.162249] [21:43:59.161809] [21:43:59.162292] [21:43:59.162288] [21:43:59.162316] [21:43:59.162283] [21:43:59.162344] [21:43:59.162339] [21:43:59.162368] [21:43:59.162279] [21:43:59.162400] [21:43:59.162396] [21:43:59.162424] [21:43:59.162391] [21:43:59.162452] [21:43:59.162448] [21:43:59.162476] [21:43:59.162273] [21:43:59.162519] [21:43:59.162515] [21:43:59.162544] [21:43:59.162510] [21:43:59.162573] [21:43:59.162568] [21:43:59.162596] [21:43:59.162506] [21:43:59.162629] [21:43:59.162624] [21:43:59.162652] [21:43:59.162620] [21:43:59.162680] [21:43:59.162676] [21:43:59.162704] [21:43:59.161802] [21:43:59.162750] [21:43:59.162746] [21:43:59.162774] [21:43:59.162741] [21:43:59.162802] [21:43:59.162798] [21:43:59.162826] [21:43:59.162737] [21:43:59.162859] [21:43:59.162854] [21:43:59.162883] [21:43:59.162850] [21:43:59.162912] [21:43:59.162907] [21:43:59.162935] [21:43:59.162732] [21:43:59.162973] [21:43:59.162968] [21:43:59.162996] [21:43:59.162964] [21:43:59.163025] [21:43:59.163020] [21:43:59.163049] [21:43:59.162959] [21:43:59.163082] [21:43:59.163077] [21:43:59.163106] [21:43:59.163073] [21:43:59.163142] [21:43:59.163137] [21:43:59.163166] [21:43:59.162728] [21:43:59.163219] [21:43:59.163215] [21:43:59.163245] [21:43:59.163210] [21:43:59.163274] [21:43:59.163269] [21:43:59.163298] [21:43:59.163206] [21:43:59.163331] [21:43:59.163326] [21:43:59.163354] [21:43:59.163322] [21:43:59.163383] [21:43:59.163378] [21:43:59.163406] [21:43:59.163200] [21:43:59.163444] [21:43:59.163440] [21:43:59.163468] [21:43:59.163435] [21:43:59.163496] [21:43:59.163492] [21:43:59.163526] [21:43:59.163430] [21:43:59.163559] [21:43:59.163555] [21:43:59.163583] [21:43:59.163550] [21:43:59.163611] [21:43:59.163607] [21:43:59.163635] first 20 keys:\n",
      "[21:43:59.198989] [21:43:59.198985] [21:43:59.199028] [21:43:59.198980] [21:43:59.199059] [21:43:59.199054] [21:43:59.199084] [21:43:59.198975] [21:43:59.199118] [21:43:59.199113] [21:43:59.199142] [21:43:59.199109] [21:43:59.199170] [21:43:59.199166] [21:43:59.199195] [21:43:59.198971] [21:43:59.199232] [21:43:59.199228] [21:43:59.199256] [21:43:59.199224] [21:43:59.199284] [21:43:59.199279] [21:43:59.199307] [21:43:59.199219] [21:43:59.199342] [21:43:59.199337] [21:43:59.199366] [21:43:59.199333] [21:43:59.199394] [21:43:59.199390] [21:43:59.199418] [21:43:59.198965] [21:43:59.199459] [21:43:59.199455] [21:43:59.199483] [21:43:59.199450] [21:43:59.199518] [21:43:59.199513] [21:43:59.199543] [21:43:59.199446] [21:43:59.199577] [21:43:59.199572] [21:43:59.199600] [21:43:59.199567] [21:43:59.199627] [21:43:59.199623] [21:43:59.199651] [21:43:59.199441] [21:43:59.199688] [21:43:59.199683] [21:43:59.199712] [21:43:59.199679] [21:43:59.199740] [21:43:59.199735] [21:43:59.199763] [21:43:59.199674] [21:43:59.199797] [21:43:59.199792] [21:43:59.199820] [21:43:59.199787] [21:43:59.199848] [21:43:59.199843] [21:43:59.199871] [21:43:59.198953] [21:43:59.199917] [21:43:59.199913] [21:43:59.199942] [21:43:59.199909] [21:43:59.199970] [21:43:59.199966] [21:43:59.199995] [21:43:59.199904] [21:43:59.200029] [21:43:59.200024] [21:43:59.200053] [21:43:59.200020] [21:43:59.200081] [21:43:59.200077] [21:43:59.200105] [21:43:59.199900] [21:43:59.200142] [21:43:59.200137] [21:43:59.200165] [21:43:59.200133] [21:43:59.200193] [21:43:59.200189] [21:43:59.200216] [21:43:59.200128] [21:43:59.200249] [21:43:59.200244] [21:43:59.200272] [21:43:59.200240] [21:43:59.200300] [21:43:59.200296] [21:43:59.200324] [21:43:59.199895] [21:43:59.200366] [21:43:59.200362] [21:43:59.200390] [21:43:59.200357] [21:43:59.200419] [21:43:59.200414] [21:43:59.200442] [21:43:59.200353] [21:43:59.200474] [21:43:59.200470] [21:43:59.200497] [21:43:59.200465] [21:43:59.200531] [21:43:59.200527] [21:43:59.200555] [21:43:59.200348] [21:43:59.200593] [21:43:59.200589] [21:43:59.200617] [21:43:59.200584] [21:43:59.200645] [21:43:59.200640] [21:43:59.200668] [21:43:59.200580] [21:43:59.200700] [21:43:59.200696] [21:43:59.200724] [21:43:59.200691] [21:43:59.200751] [21:43:59.200747] [21:43:59.200775] betas torch.Size([1000])\n",
      "[21:43:59.206299] [21:43:59.206294] [21:43:59.206327] [21:43:59.206290] [21:43:59.206358] [21:43:59.206354] [21:43:59.206382] [21:43:59.206285] [21:43:59.206416] [21:43:59.206411] [21:43:59.206439] [21:43:59.206407] [21:43:59.206467] [21:43:59.206462] [21:43:59.206490] [21:43:59.206281] [21:43:59.206535] [21:43:59.206530] [21:43:59.206559] [21:43:59.206526] [21:43:59.206589] [21:43:59.206584] [21:43:59.206613] [21:43:59.206520] [21:43:59.206645] [21:43:59.206641] [21:43:59.206668] [21:43:59.206636] [21:43:59.206696] [21:43:59.206692] [21:43:59.206719] [21:43:59.206276] [21:43:59.206760] [21:43:59.206756] [21:43:59.206783] [21:43:59.206752] [21:43:59.206811] [21:43:59.206807] [21:43:59.206834] [21:43:59.206747] [21:43:59.206866] [21:43:59.206862] [21:43:59.206890] [21:43:59.206857] [21:43:59.206917] [21:43:59.206913] [21:43:59.206940] [21:43:59.206743] [21:43:59.206977] [21:43:59.206973] [21:43:59.207001] [21:43:59.206968] [21:43:59.207028] [21:43:59.207023] [21:43:59.207051] [21:43:59.206964] [21:43:59.207083] [21:43:59.207078] [21:43:59.207106] [21:43:59.207074] [21:43:59.207133] [21:43:59.207129] [21:43:59.207156] [21:43:59.206266] [21:43:59.207202] [21:43:59.207197] [21:43:59.207225] [21:43:59.207193] [21:43:59.207253] [21:43:59.207248] [21:43:59.207277] [21:43:59.207189] [21:43:59.207312] [21:43:59.207308] [21:43:59.207335] [21:43:59.207303] [21:43:59.207363] [21:43:59.207358] [21:43:59.207386] [21:43:59.207184] [21:43:59.207423] [21:43:59.207418] [21:43:59.207446] [21:43:59.207414] [21:43:59.207473] [21:43:59.207469] [21:43:59.207508] [21:43:59.207409] [21:43:59.207542] [21:43:59.207537] [21:43:59.207565] [21:43:59.207532] [21:43:59.207593] [21:43:59.207588] [21:43:59.207616] [21:43:59.207180] [21:43:59.207657] [21:43:59.207653] [21:43:59.207680] [21:43:59.207648] [21:43:59.207708] [21:43:59.207704] [21:43:59.207731] [21:43:59.207644] [21:43:59.207763] [21:43:59.207759] [21:43:59.207786] [21:43:59.207754] [21:43:59.207814] [21:43:59.207809] [21:43:59.207837] [21:43:59.207639] [21:43:59.207874] [21:43:59.207869] [21:43:59.207897] [21:43:59.207865] [21:43:59.207924] [21:43:59.207920] [21:43:59.207947] [21:43:59.207860] [21:43:59.207979] [21:43:59.207975] [21:43:59.208003] [21:43:59.207970] [21:43:59.208031] [21:43:59.208026] [21:43:59.208054] alphas_cumprod torch.Size([1000])\n",
      "[21:43:59.208118] [21:43:59.208114] [21:43:59.208142] [21:43:59.208109] [21:43:59.208170] [21:43:59.208165] [21:43:59.208193] [21:43:59.208105] [21:43:59.208225] [21:43:59.208221] [21:43:59.208248] [21:43:59.208216] [21:43:59.208276] [21:43:59.208271] [21:43:59.208299] [21:43:59.208101] [21:43:59.208336] [21:43:59.208331] [21:43:59.208360] [21:43:59.208327] [21:43:59.208388] [21:43:59.208383] [21:43:59.208411] [21:43:59.208322] [21:43:59.208443] [21:43:59.208438] [21:43:59.208466] [21:43:59.208434] [21:43:59.208498] [21:43:59.208493] [21:43:59.208530] [21:43:59.208096] [21:43:59.208571] [21:43:59.208567] [21:43:59.208594] [21:43:59.208562] [21:43:59.208621] [21:43:59.208617] [21:43:59.208644] [21:43:59.208558] [21:43:59.208676] [21:43:59.208671] [21:43:59.208700] [21:43:59.208667] [21:43:59.208728] [21:43:59.208723] [21:43:59.208751] [21:43:59.208553] [21:43:59.208788] [21:43:59.208783] [21:43:59.208811] [21:43:59.208779] [21:43:59.208839] [21:43:59.208834] [21:43:59.208862] [21:43:59.208774] [21:43:59.208894] [21:43:59.208889] [21:43:59.208916] [21:43:59.208885] [21:43:59.208944] [21:43:59.208939] [21:43:59.208967] [21:43:59.208091] [21:43:59.209013] [21:43:59.209008] [21:43:59.209036] [21:43:59.209004] [21:43:59.209063] [21:43:59.209059] [21:43:59.209086] [21:43:59.208999] [21:43:59.209119] [21:43:59.209114] [21:43:59.209142] [21:43:59.209110] [21:43:59.209170] [21:43:59.209165] [21:43:59.209192] [21:43:59.208995] [21:43:59.209229] [21:43:59.209225] [21:43:59.209252] [21:43:59.209220] [21:43:59.209280] [21:43:59.209275] [21:43:59.209303] [21:43:59.209216] [21:43:59.209335] [21:43:59.209330] [21:43:59.209358] [21:43:59.209326] [21:43:59.209385] [21:43:59.209381] [21:43:59.209409] [21:43:59.208991] [21:43:59.209450] [21:43:59.209446] [21:43:59.209473] [21:43:59.209441] [21:43:59.209511] [21:43:59.209496] [21:43:59.209534] [21:43:59.209437] [21:43:59.209567] [21:43:59.209562] [21:43:59.209590] [21:43:59.209558] [21:43:59.209618] [21:43:59.209614] [21:43:59.209641] [21:43:59.209432] [21:43:59.209678] [21:43:59.209673] [21:43:59.209701] [21:43:59.209669] [21:43:59.209728] [21:43:59.209724] [21:43:59.209751] [21:43:59.209664] [21:43:59.209783] [21:43:59.209779] [21:43:59.209806] [21:43:59.209774] [21:43:59.209834] [21:43:59.209829] [21:43:59.209856] alphas_cumprod_prev torch.Size([1000])\n",
      "[21:43:59.209918] [21:43:59.209913] [21:43:59.209941] [21:43:59.209909] [21:43:59.209969] [21:43:59.209965] [21:43:59.209992] [21:43:59.209905] [21:43:59.210024] [21:43:59.210020] [21:43:59.210047] [21:43:59.210015] [21:43:59.210074] [21:43:59.210070] [21:43:59.210098] [21:43:59.209900] [21:43:59.210135] [21:43:59.210130] [21:43:59.210158] [21:43:59.210126] [21:43:59.210195] [21:43:59.210186] [21:43:59.210223] [21:43:59.210121] [21:43:59.210256] [21:43:59.210251] [21:43:59.210279] [21:43:59.210247] [21:43:59.210307] [21:43:59.210302] [21:43:59.210329] [21:43:59.209896] [21:43:59.210370] [21:43:59.210366] [21:43:59.210394] [21:43:59.210362] [21:43:59.210421] [21:43:59.210417] [21:43:59.210444] [21:43:59.210357] [21:43:59.210477] [21:43:59.210472] [21:43:59.210508] [21:43:59.210468] [21:43:59.210537] [21:43:59.210532] [21:43:59.210561] [21:43:59.210353] [21:43:59.210598] [21:43:59.210594] [21:43:59.210622] [21:43:59.210588] [21:43:59.210649] [21:43:59.210645] [21:43:59.210672] [21:43:59.210584] [21:43:59.210704] [21:43:59.210699] [21:43:59.210727] [21:43:59.210695] [21:43:59.210754] [21:43:59.210750] [21:43:59.210777] [21:43:59.209891] [21:43:59.210822] [21:43:59.210818] [21:43:59.210845] [21:43:59.210813] [21:43:59.210872] [21:43:59.210868] [21:43:59.210895] [21:43:59.210809] [21:43:59.210927] [21:43:59.210923] [21:43:59.210950] [21:43:59.210918] [21:43:59.210978] [21:43:59.210973] [21:43:59.211001] [21:43:59.210805] [21:43:59.211038] [21:43:59.211033] [21:43:59.211062] [21:43:59.211029] [21:43:59.211089] [21:43:59.211084] [21:43:59.211112] [21:43:59.211024] [21:43:59.211144] [21:43:59.211139] [21:43:59.211167] [21:43:59.211135] [21:43:59.211194] [21:43:59.211189] [21:43:59.211217] [21:43:59.210800] [21:43:59.211258] [21:43:59.211254] [21:43:59.211281] [21:43:59.211249] [21:43:59.211308] [21:43:59.211304] [21:43:59.211331] [21:43:59.211245] [21:43:59.211363] [21:43:59.211359] [21:43:59.211386] [21:43:59.211354] [21:43:59.211413] [21:43:59.211409] [21:43:59.211436] [21:43:59.211240] [21:43:59.211472] [21:43:59.211467] [21:43:59.211495] [21:43:59.211463] [21:43:59.211530] [21:43:59.211525] [21:43:59.211553] [21:43:59.211459] [21:43:59.211585] [21:43:59.211580] [21:43:59.211607] [21:43:59.211576] [21:43:59.211636] [21:43:59.211631] [21:43:59.211659] sqrt_alphas_cumprod torch.Size([1000])\n",
      "[21:43:59.211724] [21:43:59.211719] [21:43:59.211748] [21:43:59.211715] [21:43:59.211776] [21:43:59.211772] [21:43:59.211799] [21:43:59.211711] [21:43:59.211832] [21:43:59.211827] [21:43:59.211855] [21:43:59.211823] [21:43:59.211882] [21:43:59.211877] [21:43:59.211905] [21:43:59.211706] [21:43:59.211941] [21:43:59.211936] [21:43:59.211963] [21:43:59.211932] [21:43:59.211991] [21:43:59.211986] [21:43:59.212013] [21:43:59.211928] [21:43:59.212045] [21:43:59.212041] [21:43:59.212068] [21:43:59.212036] [21:43:59.212096] [21:43:59.212091] [21:43:59.212118] [21:43:59.211702] [21:43:59.212159] [21:43:59.212155] [21:43:59.212182] [21:43:59.212150] [21:43:59.212211] [21:43:59.212206] [21:43:59.212234] [21:43:59.212146] [21:43:59.212266] [21:43:59.212262] [21:43:59.212289] [21:43:59.212257] [21:43:59.212317] [21:43:59.212312] [21:43:59.212340] [21:43:59.212141] [21:43:59.212376] [21:43:59.212372] [21:43:59.212400] [21:43:59.212368] [21:43:59.212427] [21:43:59.212423] [21:43:59.212450] [21:43:59.212363] [21:43:59.212482] [21:43:59.212478] [21:43:59.212513] [21:43:59.212473] [21:43:59.212542] [21:43:59.212538] [21:43:59.212566] [21:43:59.211697] [21:43:59.212612] [21:43:59.212608] [21:43:59.212635] [21:43:59.212603] [21:43:59.212663] [21:43:59.212659] [21:43:59.212686] [21:43:59.212599] [21:43:59.212719] [21:43:59.212715] [21:43:59.212743] [21:43:59.212710] [21:43:59.212771] [21:43:59.212766] [21:43:59.212794] [21:43:59.212594] [21:43:59.212831] [21:43:59.212827] [21:43:59.212855] [21:43:59.212822] [21:43:59.212883] [21:43:59.212878] [21:43:59.212906] [21:43:59.212818] [21:43:59.212939] [21:43:59.212934] [21:43:59.212962] [21:43:59.212930] [21:43:59.212990] [21:43:59.212986] [21:43:59.213014] [21:43:59.212590] [21:43:59.213055] [21:43:59.213051] [21:43:59.213078] [21:43:59.213046] [21:43:59.213106] [21:43:59.213102] [21:43:59.213129] [21:43:59.213042] [21:43:59.213161] [21:43:59.213157] [21:43:59.213184] [21:43:59.213152] [21:43:59.213212] [21:43:59.213207] [21:43:59.213235] [21:43:59.213038] [21:43:59.213272] [21:43:59.213267] [21:43:59.213295] [21:43:59.213263] [21:43:59.213323] [21:43:59.213319] [21:43:59.213347] [21:43:59.213258] [21:43:59.213379] [21:43:59.213375] [21:43:59.213403] [21:43:59.213370] [21:43:59.213431] [21:43:59.213426] [21:43:59.213454] sqrt_one_minus_alphas_cumprod torch.Size([1000])\n",
      "[21:43:59.213527] [21:43:59.213521] [21:43:59.213552] [21:43:59.213517] [21:43:59.213580] [21:43:59.213575] [21:43:59.213604] [21:43:59.213513] [21:43:59.213637] [21:43:59.213632] [21:43:59.213660] [21:43:59.213628] [21:43:59.213688] [21:43:59.213684] [21:43:59.213711] [21:43:59.213508] [21:43:59.213889] [21:43:59.213743] [21:43:59.213912] [21:43:59.213739] [21:43:59.213940] [21:43:59.213935] [21:43:59.213963] [21:43:59.213735] [21:43:59.213995] [21:43:59.213990] [21:43:59.214018] [21:43:59.213986] [21:43:59.214045] [21:43:59.214041] [21:43:59.214068] [21:43:59.213496] [21:43:59.214109] [21:43:59.214105] [21:43:59.214132] [21:43:59.214100] [21:43:59.214160] [21:43:59.214155] [21:43:59.214183] [21:43:59.214096] [21:43:59.214215] [21:43:59.214211] [21:43:59.214238] [21:43:59.214206] [21:43:59.214266] [21:43:59.214261] [21:43:59.214289] [21:43:59.214092] [21:43:59.214325] [21:43:59.214321] [21:43:59.214348] [21:43:59.214316] [21:43:59.214376] [21:43:59.214372] [21:43:59.214399] [21:43:59.214312] [21:43:59.214455] [21:43:59.214451] [21:43:59.214478] [21:43:59.214446] [21:43:59.214520] [21:43:59.214516] [21:43:59.214544] [21:43:59.213491] [21:43:59.214589] [21:43:59.214585] [21:43:59.214612] [21:43:59.214581] [21:43:59.214640] [21:43:59.214636] [21:43:59.214663] [21:43:59.214576] [21:43:59.214696] [21:43:59.214691] [21:43:59.214719] [21:43:59.214687] [21:43:59.214746] [21:43:59.214742] [21:43:59.214769] [21:43:59.214572] [21:43:59.214806] [21:43:59.214802] [21:43:59.214830] [21:43:59.214797] [21:43:59.214858] [21:43:59.214853] [21:43:59.214880] [21:43:59.214793] [21:43:59.214913] [21:43:59.214908] [21:43:59.214936] [21:43:59.214904] [21:43:59.214963] [21:43:59.214959] [21:43:59.214986] [21:43:59.214567] [21:43:59.215027] [21:43:59.215023] [21:43:59.215050] [21:43:59.215018] [21:43:59.215077] [21:43:59.215073] [21:43:59.215100] [21:43:59.215014] [21:43:59.215132] [21:43:59.215128] [21:43:59.215155] [21:43:59.215123] [21:43:59.215183] [21:43:59.215178] [21:43:59.215206] [21:43:59.215009] [21:43:59.215242] [21:43:59.215238] [21:43:59.215265] [21:43:59.215234] [21:43:59.215293] [21:43:59.215288] [21:43:59.215316] [21:43:59.215229] [21:43:59.215348] [21:43:59.215343] [21:43:59.215371] [21:43:59.215339] [21:43:59.215398] [21:43:59.215394] [21:43:59.215421] log_one_minus_alphas_cumprod torch.Size([1000])\n",
      "[21:43:59.215483] [21:43:59.215478] [21:43:59.215524] [21:43:59.215474] [21:43:59.215552] [21:43:59.215548] [21:43:59.215575] [21:43:59.215470] [21:43:59.215607] [21:43:59.215602] [21:43:59.215630] [21:43:59.215598] [21:43:59.215659] [21:43:59.215654] [21:43:59.215683] [21:43:59.215465] [21:43:59.215719] [21:43:59.215715] [21:43:59.215742] [21:43:59.215711] [21:43:59.215770] [21:43:59.215766] [21:43:59.215794] [21:43:59.215706] [21:43:59.215827] [21:43:59.215822] [21:43:59.215850] [21:43:59.215818] [21:43:59.215878] [21:43:59.215874] [21:43:59.215914] [21:43:59.215461] [21:43:59.215977] [21:43:59.215970] [21:43:59.216010] [21:43:59.215964] [21:43:59.216042] [21:43:59.216037] [21:43:59.216066] [21:43:59.215957] [21:43:59.216098] [21:43:59.216094] [21:43:59.216121] [21:43:59.216089] [21:43:59.216149] [21:43:59.216144] [21:43:59.216172] [21:43:59.215950] [21:43:59.216209] [21:43:59.216204] [21:43:59.216232] [21:43:59.216200] [21:43:59.216260] [21:43:59.216255] [21:43:59.216283] [21:43:59.216195] [21:43:59.216315] [21:43:59.216310] [21:43:59.216338] [21:43:59.216306] [21:43:59.216366] [21:43:59.216361] [21:43:59.216388] [21:43:59.215456] [21:43:59.216434] [21:43:59.216430] [21:43:59.216458] [21:43:59.216425] [21:43:59.216488] [21:43:59.216483] [21:43:59.216518] [21:43:59.216421] [21:43:59.216552] [21:43:59.216547] [21:43:59.216575] [21:43:59.216543] [21:43:59.216604] [21:43:59.216598] [21:43:59.216627] [21:43:59.216416] [21:43:59.216664] [21:43:59.216659] [21:43:59.216688] [21:43:59.216655] [21:43:59.216716] [21:43:59.216711] [21:43:59.216739] [21:43:59.216650] [21:43:59.216771] [21:43:59.216767] [21:43:59.216794] [21:43:59.216762] [21:43:59.216822] [21:43:59.216817] [21:43:59.216845] [21:43:59.216412] [21:43:59.216886] [21:43:59.216881] [21:43:59.216909] [21:43:59.216877] [21:43:59.216936] [21:43:59.216932] [21:43:59.216959] [21:43:59.216873] [21:43:59.216991] [21:43:59.216987] [21:43:59.217015] [21:43:59.216982] [21:43:59.217042] [21:43:59.217038] [21:43:59.217065] [21:43:59.216868] [21:43:59.217101] [21:43:59.217097] [21:43:59.217124] [21:43:59.217093] [21:43:59.217152] [21:43:59.217147] [21:43:59.217175] [21:43:59.217088] [21:43:59.217207] [21:43:59.217202] [21:43:59.217230] [21:43:59.217198] [21:43:59.217257] [21:43:59.217253] [21:43:59.217280] sqrt_recip_alphas_cumprod torch.Size([1000])\n",
      "[21:43:59.217342] [21:43:59.217337] [21:43:59.217365] [21:43:59.217333] [21:43:59.217393] [21:43:59.217388] [21:43:59.217416] [21:43:59.217328] [21:43:59.217448] [21:43:59.217444] [21:43:59.217471] [21:43:59.217439] [21:43:59.217499] [21:43:59.217494] [21:43:59.217531] [21:43:59.217324] [21:43:59.217567] [21:43:59.217563] [21:43:59.217590] [21:43:59.217558] [21:43:59.217619] [21:43:59.217615] [21:43:59.217642] [21:43:59.217554] [21:43:59.217675] [21:43:59.217670] [21:43:59.217697] [21:43:59.217666] [21:43:59.217725] [21:43:59.217720] [21:43:59.217747] [21:43:59.217320] [21:43:59.217788] [21:43:59.217784] [21:43:59.217811] [21:43:59.217779] [21:43:59.217838] [21:43:59.217833] [21:43:59.217861] [21:43:59.217775] [21:43:59.217893] [21:43:59.217888] [21:43:59.217916] [21:43:59.217884] [21:43:59.217943] [21:43:59.217939] [21:43:59.217966] [21:43:59.217770] [21:43:59.218002] [21:43:59.217998] [21:43:59.218025] [21:43:59.217993] [21:43:59.218052] [21:43:59.218048] [21:43:59.218075] [21:43:59.217989] [21:43:59.218107] [21:43:59.218103] [21:43:59.218130] [21:43:59.218099] [21:43:59.218157] [21:43:59.218153] [21:43:59.218180] [21:43:59.217315] [21:43:59.218225] [21:43:59.218221] [21:43:59.218248] [21:43:59.218216] [21:43:59.218275] [21:43:59.218271] [21:43:59.218298] [21:43:59.218212] [21:43:59.218330] [21:43:59.218326] [21:43:59.218358] [21:43:59.218321] [21:43:59.218392] [21:43:59.218387] [21:43:59.218416] [21:43:59.218208] [21:43:59.218453] [21:43:59.218448] [21:43:59.218476] [21:43:59.218444] [21:43:59.218508] [21:43:59.218504] [21:43:59.218532] [21:43:59.218439] [21:43:59.218564] [21:43:59.218560] [21:43:59.218587] [21:43:59.218555] [21:43:59.218615] [21:43:59.218610] [21:43:59.218638] [21:43:59.218203] [21:43:59.218678] [21:43:59.218674] [21:43:59.218710] [21:43:59.218670] [21:43:59.218737] [21:43:59.218733] [21:43:59.218760] [21:43:59.218665] [21:43:59.218792] [21:43:59.218788] [21:43:59.218815] [21:43:59.218783] [21:43:59.218842] [21:43:59.218838] [21:43:59.218865] [21:43:59.218661] [21:43:59.218901] [21:43:59.218897] [21:43:59.218924] [21:43:59.218892] [21:43:59.218951] [21:43:59.218947] [21:43:59.218974] [21:43:59.218888] [21:43:59.219006] [21:43:59.219002] [21:43:59.219029] [21:43:59.218997] [21:43:59.219056] [21:43:59.219051] [21:43:59.219079] sqrt_recipm1_alphas_cumprod torch.Size([1000])\n",
      "[21:43:59.219139] [21:43:59.219135] [21:43:59.219162] [21:43:59.219131] [21:43:59.219190] [21:43:59.219186] [21:43:59.219213] [21:43:59.219126] [21:43:59.219244] [21:43:59.219240] [21:43:59.219266] [21:43:59.219235] [21:43:59.219293] [21:43:59.219289] [21:43:59.219316] [21:43:59.219122] [21:43:59.219352] [21:43:59.219348] [21:43:59.219374] [21:43:59.219343] [21:43:59.219402] [21:43:59.219397] [21:43:59.219424] [21:43:59.219339] [21:43:59.219457] [21:43:59.219452] [21:43:59.219480] [21:43:59.219448] [21:43:59.219514] [21:43:59.219509] [21:43:59.219537] [21:43:59.219118] [21:43:59.219578] [21:43:59.219574] [21:43:59.219601] [21:43:59.219569] [21:43:59.219628] [21:43:59.219624] [21:43:59.219651] [21:43:59.219565] [21:43:59.219683] [21:43:59.219678] [21:43:59.219706] [21:43:59.219674] [21:43:59.219733] [21:43:59.219729] [21:43:59.219756] [21:43:59.219560] [21:43:59.219792] [21:43:59.219788] [21:43:59.219815] [21:43:59.219783] [21:43:59.219842] [21:43:59.219837] [21:43:59.219864] [21:43:59.219779] [21:43:59.219896] [21:43:59.219892] [21:43:59.219919] [21:43:59.219887] [21:43:59.219946] [21:43:59.219941] [21:43:59.219969] [21:43:59.219113] [21:43:59.220014] [21:43:59.220009] [21:43:59.220037] [21:43:59.220005] [21:43:59.220064] [21:43:59.220060] [21:43:59.220087] [21:43:59.220000] [21:43:59.220119] [21:43:59.220115] [21:43:59.220142] [21:43:59.220110] [21:43:59.220169] [21:43:59.220165] [21:43:59.220192] [21:43:59.219996] [21:43:59.220228] [21:43:59.220224] [21:43:59.220251] [21:43:59.220220] [21:43:59.220278] [21:43:59.220273] [21:43:59.220300] [21:43:59.220215] [21:43:59.220332] [21:43:59.220328] [21:43:59.220355] [21:43:59.220323] [21:43:59.220382] [21:43:59.220378] [21:43:59.220405] [21:43:59.219992] [21:43:59.220446] [21:43:59.220441] [21:43:59.220468] [21:43:59.220437] [21:43:59.220495] [21:43:59.220491] [21:43:59.220528] [21:43:59.220433] [21:43:59.220561] [21:43:59.220556] [21:43:59.220583] [21:43:59.220552] [21:43:59.220611] [21:43:59.220606] [21:43:59.220633] [21:43:59.220428] [21:43:59.220670] [21:43:59.220665] [21:43:59.220692] [21:43:59.220661] [21:43:59.220720] [21:43:59.220715] [21:43:59.220742] [21:43:59.220656] [21:43:59.220774] [21:43:59.220770] [21:43:59.220797] [21:43:59.220766] [21:43:59.220824] [21:43:59.220820] [21:43:59.220847] posterior_variance torch.Size([1000])\n",
      "[21:43:59.220907] [21:43:59.220903] [21:43:59.220931] [21:43:59.220899] [21:43:59.220958] [21:43:59.220954] [21:43:59.220980] [21:43:59.220895] [21:43:59.221012] [21:43:59.221008] [21:43:59.221034] [21:43:59.221003] [21:43:59.221061] [21:43:59.221057] [21:43:59.221084] [21:43:59.220890] [21:43:59.221122] [21:43:59.221118] [21:43:59.221144] [21:43:59.221113] [21:43:59.221172] [21:43:59.221167] [21:43:59.221194] [21:43:59.221109] [21:43:59.221226] [21:43:59.221221] [21:43:59.221248] [21:43:59.221217] [21:43:59.221275] [21:43:59.221271] [21:43:59.221298] [21:43:59.220886] [21:43:59.221338] [21:43:59.221334] [21:43:59.221361] [21:43:59.221329] [21:43:59.221388] [21:43:59.221383] [21:43:59.221410] [21:43:59.221325] [21:43:59.221442] [21:43:59.221437] [21:43:59.221464] [21:43:59.221433] [21:43:59.221491] [21:43:59.221487] [21:43:59.221524] [21:43:59.221321] [21:43:59.221564] [21:43:59.221559] [21:43:59.221588] [21:43:59.221555] [21:43:59.221616] [21:43:59.221611] [21:43:59.221640] [21:43:59.221550] [21:43:59.221672] [21:43:59.221668] [21:43:59.221695] [21:43:59.221663] [21:43:59.221723] [21:43:59.221718] [21:43:59.221746] [21:43:59.220881] [21:43:59.221791] [21:43:59.221787] [21:43:59.221814] [21:43:59.221782] [21:43:59.221842] [21:43:59.221837] [21:43:59.221865] [21:43:59.221778] [21:43:59.221897] [21:43:59.221892] [21:43:59.221920] [21:43:59.221888] [21:43:59.221948] [21:43:59.221943] [21:43:59.221970] [21:43:59.221773] [21:43:59.222007] [21:43:59.222003] [21:43:59.222030] [21:43:59.221998] [21:43:59.222058] [21:43:59.222053] [21:43:59.222081] [21:43:59.221994] [21:43:59.222113] [21:43:59.222109] [21:43:59.222136] [21:43:59.222104] [21:43:59.222164] [21:43:59.222160] [21:43:59.222187] [21:43:59.221769] [21:43:59.222228] [21:43:59.222224] [21:43:59.222252] [21:43:59.222219] [21:43:59.222280] [21:43:59.222275] [21:43:59.222303] [21:43:59.222215] [21:43:59.222335] [21:43:59.222331] [21:43:59.222358] [21:43:59.222327] [21:43:59.222386] [21:43:59.222382] [21:43:59.222409] [21:43:59.222211] [21:43:59.222447] [21:43:59.222442] [21:43:59.222470] [21:43:59.222437] [21:43:59.222498] [21:43:59.222493] [21:43:59.222532] [21:43:59.222433] [21:43:59.222564] [21:43:59.222559] [21:43:59.222587] [21:43:59.222555] [21:43:59.222615] [21:43:59.222610] [21:43:59.222638] posterior_log_variance_clipped torch.Size([1000])\n",
      "[21:43:59.222698] [21:43:59.222694] [21:43:59.222722] [21:43:59.222689] [21:43:59.222749] [21:43:59.222745] [21:43:59.222772] [21:43:59.222685] [21:43:59.222804] [21:43:59.222800] [21:43:59.222827] [21:43:59.222795] [21:43:59.222854] [21:43:59.222850] [21:43:59.222877] [21:43:59.222681] [21:43:59.222914] [21:43:59.222909] [21:43:59.222938] [21:43:59.222905] [21:43:59.222965] [21:43:59.222961] [21:43:59.222989] [21:43:59.222900] [21:43:59.223021] [21:43:59.223017] [21:43:59.223044] [21:43:59.223012] [21:43:59.223071] [21:43:59.223067] [21:43:59.223094] [21:43:59.222676] [21:43:59.223135] [21:43:59.223131] [21:43:59.223158] [21:43:59.223126] [21:43:59.223186] [21:43:59.223181] [21:43:59.223208] [21:43:59.223122] [21:43:59.223240] [21:43:59.223236] [21:43:59.223263] [21:43:59.223231] [21:43:59.223291] [21:43:59.223287] [21:43:59.223314] [21:43:59.223118] [21:43:59.223350] [21:43:59.223346] [21:43:59.223373] [21:43:59.223342] [21:43:59.223401] [21:43:59.223396] [21:43:59.223424] [21:43:59.223337] [21:43:59.223455] [21:43:59.223451] [21:43:59.223478] [21:43:59.223446] [21:43:59.223512] [21:43:59.223507] [21:43:59.223536] [21:43:59.222672] [21:43:59.223581] [21:43:59.223577] [21:43:59.223605] [21:43:59.223573] [21:43:59.223633] [21:43:59.223628] [21:43:59.223656] [21:43:59.223569] [21:43:59.223688] [21:43:59.223684] [21:43:59.223711] [21:43:59.223679] [21:43:59.223739] [21:43:59.223734] [21:43:59.223762] [21:43:59.223564] [21:43:59.223799] [21:43:59.223794] [21:43:59.223821] [21:43:59.223790] [21:43:59.223849] [21:43:59.223845] [21:43:59.223872] [21:43:59.223785] [21:43:59.223905] [21:43:59.223900] [21:43:59.223928] [21:43:59.223896] [21:43:59.223955] [21:43:59.223951] [21:43:59.223978] [21:43:59.223560] [21:43:59.224019] [21:43:59.224015] [21:43:59.224042] [21:43:59.224010] [21:43:59.224069] [21:43:59.224065] [21:43:59.224092] [21:43:59.224006] [21:43:59.224124] [21:43:59.224120] [21:43:59.224148] [21:43:59.224115] [21:43:59.224176] [21:43:59.224171] [21:43:59.224199] [21:43:59.224002] [21:43:59.224236] [21:43:59.224231] [21:43:59.224259] [21:43:59.224227] [21:43:59.224287] [21:43:59.224283] [21:43:59.224310] [21:43:59.224223] [21:43:59.224343] [21:43:59.224338] [21:43:59.224367] [21:43:59.224334] [21:43:59.224394] [21:43:59.224390] [21:43:59.224417] posterior_mean_coef1 torch.Size([1000])\n",
      "[21:43:59.224480] [21:43:59.224475] [21:43:59.224508] [21:43:59.224471] [21:43:59.224537] [21:43:59.224532] [21:43:59.224560] [21:43:59.224467] [21:43:59.224592] [21:43:59.224588] [21:43:59.224615] [21:43:59.224583] [21:43:59.224643] [21:43:59.224638] [21:43:59.224666] [21:43:59.224462] [21:43:59.224703] [21:43:59.224698] [21:43:59.224725] [21:43:59.224694] [21:43:59.224753] [21:43:59.224748] [21:43:59.224776] [21:43:59.224689] [21:43:59.224808] [21:43:59.224804] [21:43:59.224831] [21:43:59.224799] [21:43:59.224859] [21:43:59.224854] [21:43:59.224881] [21:43:59.224458] [21:43:59.224922] [21:43:59.224918] [21:43:59.224945] [21:43:59.224914] [21:43:59.224973] [21:43:59.224968] [21:43:59.224996] [21:43:59.224909] [21:43:59.225029] [21:43:59.225024] [21:43:59.225053] [21:43:59.225020] [21:43:59.225080] [21:43:59.225076] [21:43:59.225104] [21:43:59.224905] [21:43:59.225140] [21:43:59.225136] [21:43:59.225163] [21:43:59.225132] [21:43:59.225191] [21:43:59.225187] [21:43:59.225215] [21:43:59.225127] [21:43:59.225247] [21:43:59.225242] [21:43:59.225270] [21:43:59.225238] [21:43:59.225298] [21:43:59.225293] [21:43:59.225321] [21:43:59.224453] [21:43:59.225366] [21:43:59.225362] [21:43:59.225390] [21:43:59.225358] [21:43:59.225417] [21:43:59.225413] [21:43:59.225440] [21:43:59.225353] [21:43:59.225472] [21:43:59.225468] [21:43:59.225495] [21:43:59.225463] [21:43:59.225530] [21:43:59.225525] [21:43:59.225554] [21:43:59.225349] [21:43:59.225591] [21:43:59.225586] [21:43:59.225614] [21:43:59.225582] [21:43:59.225641] [21:43:59.225637] [21:43:59.225664] [21:43:59.225577] [21:43:59.225697] [21:43:59.225692] [21:43:59.225720] [21:43:59.225688] [21:43:59.225748] [21:43:59.225743] [21:43:59.225772] [21:43:59.225344] [21:43:59.225813] [21:43:59.225808] [21:43:59.225836] [21:43:59.225804] [21:43:59.225863] [21:43:59.225859] [21:43:59.225886] [21:43:59.225800] [21:43:59.225918] [21:43:59.225914] [21:43:59.225941] [21:43:59.225909] [21:43:59.225969] [21:43:59.225964] [21:43:59.225992] [21:43:59.225795] [21:43:59.226029] [21:43:59.226024] [21:43:59.226052] [21:43:59.226020] [21:43:59.226079] [21:43:59.226075] [21:43:59.226102] [21:43:59.226015] [21:43:59.226134] [21:43:59.226130] [21:43:59.226157] [21:43:59.226125] [21:43:59.226184] [21:43:59.226180] [21:43:59.226208] posterior_mean_coef2 torch.Size([1000])\n",
      "[21:43:59.226269] [21:43:59.226264] [21:43:59.226292] [21:43:59.226260] [21:43:59.226320] [21:43:59.226316] [21:43:59.226344] [21:43:59.226256] [21:43:59.226376] [21:43:59.226371] [21:43:59.226399] [21:43:59.226367] [21:43:59.226427] [21:43:59.226423] [21:43:59.226451] [21:43:59.226251] [21:43:59.226487] [21:43:59.226483] [21:43:59.226520] [21:43:59.226479] [21:43:59.226548] [21:43:59.226544] [21:43:59.226572] [21:43:59.226474] [21:43:59.226604] [21:43:59.226599] [21:43:59.226627] [21:43:59.226595] [21:43:59.226656] [21:43:59.226651] [21:43:59.226679] [21:43:59.226247] [21:43:59.226720] [21:43:59.226716] [21:43:59.226744] [21:43:59.226712] [21:43:59.226772] [21:43:59.226767] [21:43:59.226795] [21:43:59.226707] [21:43:59.226827] [21:43:59.226823] [21:43:59.226851] [21:43:59.226818] [21:43:59.226879] [21:43:59.226874] [21:43:59.226902] [21:43:59.226703] [21:43:59.226939] [21:43:59.226934] [21:43:59.226962] [21:43:59.226930] [21:43:59.226990] [21:43:59.226986] [21:43:59.227018] [21:43:59.226925] [21:43:59.227051] [21:43:59.227046] [21:43:59.227074] [21:43:59.227042] [21:43:59.227102] [21:43:59.227097] [21:43:59.227125] [21:43:59.226242] [21:43:59.227171] [21:43:59.227167] [21:43:59.227195] [21:43:59.227163] [21:43:59.227223] [21:43:59.227218] [21:43:59.227246] [21:43:59.227158] [21:43:59.227279] [21:43:59.227274] [21:43:59.227302] [21:43:59.227270] [21:43:59.227330] [21:43:59.227326] [21:43:59.227353] [21:43:59.227153] [21:43:59.227391] [21:43:59.227386] [21:43:59.227414] [21:43:59.227382] [21:43:59.227441] [21:43:59.227437] [21:43:59.227465] [21:43:59.227377] [21:43:59.227497] [21:43:59.227492] [21:43:59.227529] [21:43:59.227488] [21:43:59.227558] [21:43:59.227554] [21:43:59.227581] [21:43:59.227149] [21:43:59.227623] [21:43:59.227618] [21:43:59.227646] [21:43:59.227614] [21:43:59.227674] [21:43:59.227670] [21:43:59.227697] [21:43:59.227610] [21:43:59.227730] [21:43:59.227725] [21:43:59.227753] [21:43:59.227721] [21:43:59.227780] [21:43:59.227776] [21:43:59.227804] [21:43:59.227605] [21:43:59.227841] [21:43:59.227836] [21:43:59.227864] [21:43:59.227832] [21:43:59.227892] [21:43:59.227888] [21:43:59.227915] [21:43:59.227827] [21:43:59.227948] [21:43:59.227943] [21:43:59.227971] [21:43:59.227939] [21:43:59.227999] [21:43:59.227995] [21:43:59.228022] model.diffusion_model.time_embed.0.weight torch.Size([256, 1536])\n",
      "[21:43:59.228432] [21:43:59.228427] [21:43:59.228457] [21:43:59.228423] [21:43:59.228486] [21:43:59.228481] [21:43:59.228523] [21:43:59.228419] [21:43:59.228556] [21:43:59.228552] [21:43:59.228579] [21:43:59.228547] [21:43:59.228608] [21:43:59.228603] [21:43:59.228632] [21:43:59.228414] [21:43:59.228669] [21:43:59.228664] [21:43:59.228692] [21:43:59.228660] [21:43:59.228720] [21:43:59.228716] [21:43:59.228743] [21:43:59.228655] [21:43:59.228776] [21:43:59.228771] [21:43:59.228799] [21:43:59.228766] [21:43:59.228829] [21:43:59.228824] [21:43:59.228852] [21:43:59.228410] [21:43:59.228893] [21:43:59.228888] [21:43:59.228916] [21:43:59.228884] [21:43:59.228944] [21:43:59.228939] [21:43:59.228967] [21:43:59.228880] [21:43:59.229000] [21:43:59.228995] [21:43:59.229023] [21:43:59.228991] [21:43:59.229051] [21:43:59.229046] [21:43:59.229074] [21:43:59.228875] [21:43:59.229111] [21:43:59.229106] [21:43:59.229134] [21:43:59.229102] [21:43:59.229162] [21:43:59.229157] [21:43:59.229185] [21:43:59.229097] [21:43:59.229217] [21:43:59.229213] [21:43:59.229240] [21:43:59.229208] [21:43:59.229268] [21:43:59.229263] [21:43:59.229291] [21:43:59.228404] [21:43:59.229336] [21:43:59.229332] [21:43:59.229359] [21:43:59.229327] [21:43:59.229387] [21:43:59.229383] [21:43:59.229410] [21:43:59.229323] [21:43:59.229442] [21:43:59.229438] [21:43:59.229465] [21:43:59.229433] [21:43:59.229493] [21:43:59.229489] [21:43:59.229530] [21:43:59.229319] [21:43:59.229567] [21:43:59.229563] [21:43:59.229590] [21:43:59.229558] [21:43:59.229619] [21:43:59.229614] [21:43:59.229643] [21:43:59.229554] [21:43:59.229676] [21:43:59.229672] [21:43:59.229699] [21:43:59.229667] [21:43:59.229727] [21:43:59.229723] [21:43:59.229751] [21:43:59.229314] [21:43:59.229793] [21:43:59.229788] [21:43:59.229817] [21:43:59.229784] [21:43:59.229845] [21:43:59.229840] [21:43:59.229868] [21:43:59.229779] [21:43:59.229901] [21:43:59.229896] [21:43:59.229924] [21:43:59.229892] [21:43:59.229952] [21:43:59.229947] [21:43:59.229975] [21:43:59.229775] [21:43:59.230012] [21:43:59.230008] [21:43:59.230035] [21:43:59.230003] [21:43:59.230063] [21:43:59.230059] [21:43:59.230087] [21:43:59.229999] [21:43:59.230119] [21:43:59.230115] [21:43:59.230142] [21:43:59.230110] [21:43:59.230170] [21:43:59.230165] [21:43:59.230193] model.diffusion_model.time_embed.0.bias torch.Size([256])\n",
      "[21:43:59.230255] [21:43:59.230251] [21:43:59.230279] [21:43:59.230246] [21:43:59.230308] [21:43:59.230303] [21:43:59.230331] [21:43:59.230242] [21:43:59.230363] [21:43:59.230359] [21:43:59.230386] [21:43:59.230354] [21:43:59.230414] [21:43:59.230409] [21:43:59.230437] [21:43:59.230238] [21:43:59.230473] [21:43:59.230469] [21:43:59.230497] [21:43:59.230464] [21:43:59.230534] [21:43:59.230529] [21:43:59.230558] [21:43:59.230460] [21:43:59.230591] [21:43:59.230586] [21:43:59.230614] [21:43:59.230581] [21:43:59.230642] [21:43:59.230637] [21:43:59.230664] [21:43:59.230233] [21:43:59.230706] [21:43:59.230702] [21:43:59.230730] [21:43:59.230697] [21:43:59.230757] [21:43:59.230753] [21:43:59.230781] [21:43:59.230692] [21:43:59.230813] [21:43:59.230809] [21:43:59.230836] [21:43:59.230804] [21:43:59.230864] [21:43:59.230860] [21:43:59.230887] [21:43:59.230688] [21:43:59.230925] [21:43:59.230921] [21:43:59.230949] [21:43:59.230917] [21:43:59.230977] [21:43:59.230972] [21:43:59.231000] [21:43:59.230912] [21:43:59.231032] [21:43:59.231027] [21:43:59.231055] [21:43:59.231023] [21:43:59.231082] [21:43:59.231078] [21:43:59.231105] [21:43:59.230228] [21:43:59.231150] [21:43:59.231146] [21:43:59.231173] [21:43:59.231142] [21:43:59.231201] [21:43:59.231196] [21:43:59.231224] [21:43:59.231137] [21:43:59.231256] [21:43:59.231252] [21:43:59.231279] [21:43:59.231247] [21:43:59.231307] [21:43:59.231303] [21:43:59.231330] [21:43:59.231133] [21:43:59.231367] [21:43:59.231363] [21:43:59.231390] [21:43:59.231358] [21:43:59.231418] [21:43:59.231414] [21:43:59.231442] [21:43:59.231354] [21:43:59.231474] [21:43:59.231470] [21:43:59.231497] [21:43:59.231465] [21:43:59.231535] [21:43:59.231531] [21:43:59.231559] [21:43:59.231129] [21:43:59.231600] [21:43:59.231596] [21:43:59.231623] [21:43:59.231591] [21:43:59.231651] [21:43:59.231646] [21:43:59.231674] [21:43:59.231587] [21:43:59.231706] [21:43:59.231701] [21:43:59.231729] [21:43:59.231697] [21:43:59.231757] [21:43:59.231752] [21:43:59.231779] [21:43:59.231583] [21:43:59.231816] [21:43:59.231812] [21:43:59.231839] [21:43:59.231807] [21:43:59.231867] [21:43:59.231862] [21:43:59.231890] [21:43:59.231803] [21:43:59.231922] [21:43:59.231917] [21:43:59.231945] [21:43:59.231913] [21:43:59.231972] [21:43:59.231968] [21:43:59.231995] model.diffusion_model.time_embed.2.weight torch.Size([256, 256])\n",
      "[21:43:59.232056] [21:43:59.232052] [21:43:59.232079] [21:43:59.232047] [21:43:59.232107] [21:43:59.232103] [21:43:59.232130] [21:43:59.232043] [21:43:59.232163] [21:43:59.232158] [21:43:59.232186] [21:43:59.232154] [21:43:59.232214] [21:43:59.232210] [21:43:59.232237] [21:43:59.232038] [21:43:59.232274] [21:43:59.232270] [21:43:59.232297] [21:43:59.232265] [21:43:59.232325] [21:43:59.232320] [21:43:59.232348] [21:43:59.232261] [21:43:59.232380] [21:43:59.232376] [21:43:59.232403] [21:43:59.232371] [21:43:59.232431] [21:43:59.232426] [21:43:59.232454] [21:43:59.232034] [21:43:59.232496] [21:43:59.232491] [21:43:59.232531] [21:43:59.232487] [21:43:59.232561] [21:43:59.232556] [21:43:59.232584] [21:43:59.232483] [21:43:59.232617] [21:43:59.232612] [21:43:59.232640] [21:43:59.232608] [21:43:59.232668] [21:43:59.232663] [21:43:59.232691] [21:43:59.232478] [21:43:59.232727] [21:43:59.232723] [21:43:59.232751] [21:43:59.232719] [21:43:59.232779] [21:43:59.232774] [21:43:59.232802] [21:43:59.232714] [21:43:59.232834] [21:43:59.232829] [21:43:59.232857] [21:43:59.232825] [21:43:59.232884] [21:43:59.232880] [21:43:59.232907] [21:43:59.232029] [21:43:59.232953] [21:43:59.232948] [21:43:59.232976] [21:43:59.232944] [21:43:59.233004] [21:43:59.232999] [21:43:59.233027] [21:43:59.232940] [21:43:59.233060] [21:43:59.233055] [21:43:59.233083] [21:43:59.233051] [21:43:59.233110] [21:43:59.233106] [21:43:59.233134] [21:43:59.232935] [21:43:59.233171] [21:43:59.233166] [21:43:59.233194] [21:43:59.233162] [21:43:59.233222] [21:43:59.233217] [21:43:59.233245] [21:43:59.233157] [21:43:59.233277] [21:43:59.233273] [21:43:59.233301] [21:43:59.233268] [21:43:59.233328] [21:43:59.233324] [21:43:59.233352] [21:43:59.232931] [21:43:59.233393] [21:43:59.233389] [21:43:59.233417] [21:43:59.233385] [21:43:59.233444] [21:43:59.233440] [21:43:59.233468] [21:43:59.233380] [21:43:59.233509] [21:43:59.233496] [21:43:59.233533] [21:43:59.233491] [21:43:59.233560] [21:43:59.233556] [21:43:59.233583] [21:43:59.233376] [21:43:59.233620] [21:43:59.233616] [21:43:59.233643] [21:43:59.233611] [21:43:59.233671] [21:43:59.233667] [21:43:59.233694] [21:43:59.233607] [21:43:59.233727] [21:43:59.233722] [21:43:59.233749] [21:43:59.233718] [21:43:59.233777] [21:43:59.233773] [21:43:59.233800] model.diffusion_model.time_embed.2.bias torch.Size([256])\n",
      "[21:43:59.233863] [21:43:59.233859] [21:43:59.233887] [21:43:59.233855] [21:43:59.233915] [21:43:59.233911] [21:43:59.233939] [21:43:59.233850] [21:43:59.233971] [21:43:59.233967] [21:43:59.233994] [21:43:59.233962] [21:43:59.234022] [21:43:59.234018] [21:43:59.234045] [21:43:59.233846] [21:43:59.234082] [21:43:59.234078] [21:43:59.234105] [21:43:59.234073] [21:43:59.234133] [21:43:59.234129] [21:43:59.234157] [21:43:59.234069] [21:43:59.234189] [21:43:59.234185] [21:43:59.234212] [21:43:59.234180] [21:43:59.234240] [21:43:59.234235] [21:43:59.234263] [21:43:59.233841] [21:43:59.234304] [21:43:59.234300] [21:43:59.234328] [21:43:59.234296] [21:43:59.234356] [21:43:59.234351] [21:43:59.234379] [21:43:59.234291] [21:43:59.234411] [21:43:59.234407] [21:43:59.234435] [21:43:59.234402] [21:43:59.234462] [21:43:59.234458] [21:43:59.234485] [21:43:59.234286] [21:43:59.234532] [21:43:59.234527] [21:43:59.234556] [21:43:59.234523] [21:43:59.234585] [21:43:59.234580] [21:43:59.234608] [21:43:59.234518] [21:43:59.234639] [21:43:59.234635] [21:43:59.234662] [21:43:59.234631] [21:43:59.234690] [21:43:59.234686] [21:43:59.234713] [21:43:59.233836] [21:43:59.234760] [21:43:59.234755] [21:43:59.234783] [21:43:59.234751] [21:43:59.234811] [21:43:59.234807] [21:43:59.234834] [21:43:59.234746] [21:43:59.234867] [21:43:59.234862] [21:43:59.234892] [21:43:59.234858] [21:43:59.234919] [21:43:59.234915] [21:43:59.234943] [21:43:59.234742] [21:43:59.234979] [21:43:59.234975] [21:43:59.235002] [21:43:59.234971] [21:43:59.235030] [21:43:59.235026] [21:43:59.235053] [21:43:59.234966] [21:43:59.235085] [21:43:59.235081] [21:43:59.235109] [21:43:59.235076] [21:43:59.235137] [21:43:59.235132] [21:43:59.235160] [21:43:59.234738] [21:43:59.235201] [21:43:59.235197] [21:43:59.235224] [21:43:59.235192] [21:43:59.235252] [21:43:59.235248] [21:43:59.235275] [21:43:59.235188] [21:43:59.235307] [21:43:59.235303] [21:43:59.235330] [21:43:59.235298] [21:43:59.235358] [21:43:59.235353] [21:43:59.235381] [21:43:59.235184] [21:43:59.235418] [21:43:59.235413] [21:43:59.235441] [21:43:59.235409] [21:43:59.235468] [21:43:59.235464] [21:43:59.235492] [21:43:59.235404] [21:43:59.235535] [21:43:59.235530] [21:43:59.235558] [21:43:59.235525] [21:43:59.235586] [21:43:59.235582] [21:43:59.235609] model.diffusion_model.input_proj.weight torch.Size([1536, 256])\n",
      "[21:43:59.235670] [21:43:59.235666] [21:43:59.235694] [21:43:59.235662] [21:43:59.235722] [21:43:59.235718] [21:43:59.235746] [21:43:59.235657] [21:43:59.235778] [21:43:59.235774] [21:43:59.235801] [21:43:59.235769] [21:43:59.235830] [21:43:59.235825] [21:43:59.235853] [21:43:59.235653] [21:43:59.235890] [21:43:59.235886] [21:43:59.235914] [21:43:59.235881] [21:43:59.235942] [21:43:59.235937] [21:43:59.235974] [21:43:59.235877] [21:43:59.236012] [21:43:59.236008] [21:43:59.236036] [21:43:59.236003] [21:43:59.236064] [21:43:59.236059] [21:43:59.236087] [21:43:59.235649] [21:43:59.236128] [21:43:59.236124] [21:43:59.236152] [21:43:59.236120] [21:43:59.236179] [21:43:59.236175] [21:43:59.236203] [21:43:59.236115] [21:43:59.236235] [21:43:59.236230] [21:43:59.236258] [21:43:59.236226] [21:43:59.236285] [21:43:59.236281] [21:43:59.236308] [21:43:59.236111] [21:43:59.236345] [21:43:59.236341] [21:43:59.236369] [21:43:59.236337] [21:43:59.236396] [21:43:59.236392] [21:43:59.236419] [21:43:59.236332] [21:43:59.236451] [21:43:59.236447] [21:43:59.236474] [21:43:59.236442] [21:43:59.236509] [21:43:59.236497] [21:43:59.236532] [21:43:59.235644] [21:43:59.236578] [21:43:59.236574] [21:43:59.236601] [21:43:59.236569] [21:43:59.236629] [21:43:59.236624] [21:43:59.236652] [21:43:59.236565] [21:43:59.236685] [21:43:59.236680] [21:43:59.236708] [21:43:59.236676] [21:43:59.236736] [21:43:59.236732] [21:43:59.236760] [21:43:59.236560] [21:43:59.236797] [21:43:59.236792] [21:43:59.236820] [21:43:59.236788] [21:43:59.236848] [21:43:59.236844] [21:43:59.236871] [21:43:59.236783] [21:43:59.236904] [21:43:59.236899] [21:43:59.236927] [21:43:59.236895] [21:43:59.236954] [21:43:59.236950] [21:43:59.236978] [21:43:59.236556] [21:43:59.237019] [21:43:59.237015] [21:43:59.237042] [21:43:59.237010] [21:43:59.237070] [21:43:59.237065] [21:43:59.237093] [21:43:59.237006] [21:43:59.237126] [21:43:59.237121] [21:43:59.237149] [21:43:59.237117] [21:43:59.237177] [21:43:59.237172] [21:43:59.237200] [21:43:59.237001] [21:43:59.237236] [21:43:59.237232] [21:43:59.237260] [21:43:59.237228] [21:43:59.237288] [21:43:59.237283] [21:43:59.237311] [21:43:59.237223] [21:43:59.237343] [21:43:59.237339] [21:43:59.237366] [21:43:59.237334] [21:43:59.237394] [21:43:59.237390] [21:43:59.237418] model.diffusion_model.input_proj.bias torch.Size([1536])\n",
      "[21:43:59.237479] [21:43:59.237475] [21:43:59.237508] [21:43:59.237471] [21:43:59.237536] [21:43:59.237532] [21:43:59.237560] [21:43:59.237466] [21:43:59.237592] [21:43:59.237587] [21:43:59.237615] [21:43:59.237583] [21:43:59.237643] [21:43:59.237638] [21:43:59.237666] [21:43:59.237462] [21:43:59.237703] [21:43:59.237699] [21:43:59.237726] [21:43:59.237694] [21:43:59.237754] [21:43:59.237750] [21:43:59.237777] [21:43:59.237690] [21:43:59.237809] [21:43:59.237805] [21:43:59.237833] [21:43:59.237800] [21:43:59.237860] [21:43:59.237856] [21:43:59.237883] [21:43:59.237458] [21:43:59.237924] [21:43:59.237920] [21:43:59.237947] [21:43:59.237915] [21:43:59.237975] [21:43:59.237970] [21:43:59.237998] [21:43:59.237911] [21:43:59.238030] [21:43:59.238025] [21:43:59.238053] [21:43:59.238021] [21:43:59.238081] [21:43:59.238076] [21:43:59.238104] [21:43:59.237906] [21:43:59.238141] [21:43:59.238137] [21:43:59.238164] [21:43:59.238132] [21:43:59.238192] [21:43:59.238188] [21:43:59.238215] [21:43:59.238128] [21:43:59.238247] [21:43:59.238243] [21:43:59.238270] [21:43:59.238238] [21:43:59.238298] [21:43:59.238294] [21:43:59.238321] [21:43:59.237453] [21:43:59.238367] [21:43:59.238362] [21:43:59.238390] [21:43:59.238358] [21:43:59.238424] [21:43:59.238416] [21:43:59.238449] [21:43:59.238353] [21:43:59.238482] [21:43:59.238477] [21:43:59.238526] [21:43:59.238473] [21:43:59.238554] [21:43:59.238549] [21:43:59.238577] [21:43:59.238349] [21:43:59.238614] [21:43:59.238610] [21:43:59.238639] [21:43:59.238605] [21:43:59.238667] [21:43:59.238662] [21:43:59.238690] [21:43:59.238601] [21:43:59.238723] [21:43:59.238718] [21:43:59.238746] [21:43:59.238714] [21:43:59.238773] [21:43:59.238769] [21:43:59.238797] [21:43:59.238344] [21:43:59.238838] [21:43:59.238834] [21:43:59.238861] [21:43:59.238829] [21:43:59.238889] [21:43:59.238885] [21:43:59.238912] [21:43:59.238825] [21:43:59.238945] [21:43:59.238940] [21:43:59.238968] [21:43:59.238936] [21:43:59.238996] [21:43:59.238992] [21:43:59.239019] [21:43:59.238820] [21:43:59.239056] [21:43:59.239051] [21:43:59.239080] [21:43:59.239047] [21:43:59.239108] [21:43:59.239104] [21:43:59.239132] [21:43:59.239043] [21:43:59.239165] [21:43:59.239160] [21:43:59.239188] [21:43:59.239156] [21:43:59.239216] [21:43:59.239211] [21:43:59.239239] model.diffusion_model.res_blocks.0.in_layers.0.weight torch.Size([1536])\n",
      "[21:43:59.239302] [21:43:59.239297] [21:43:59.239325] [21:43:59.239293] [21:43:59.239353] [21:43:59.239348] [21:43:59.239377] [21:43:59.239288] [21:43:59.239409] [21:43:59.239405] [21:43:59.239433] [21:43:59.239401] [21:43:59.239460] [21:43:59.239456] [21:43:59.239483] [21:43:59.239284] [21:43:59.239530] [21:43:59.239526] [21:43:59.239554] [21:43:59.239521] [21:43:59.239582] [21:43:59.239577] [21:43:59.239605] [21:43:59.239516] [21:43:59.239637] [21:43:59.239633] [21:43:59.239661] [21:43:59.239628] [21:43:59.239689] [21:43:59.239684] [21:43:59.239712] [21:43:59.239280] [21:43:59.239753] [21:43:59.239748] [21:43:59.239776] [21:43:59.239744] [21:43:59.239804] [21:43:59.239799] [21:43:59.239826] [21:43:59.239739] [21:43:59.239858] [21:43:59.239854] [21:43:59.239882] [21:43:59.239849] [21:43:59.239909] [21:43:59.239905] [21:43:59.239932] [21:43:59.239735] [21:43:59.239968] [21:43:59.239964] [21:43:59.239991] [21:43:59.239960] [21:43:59.240019] [21:43:59.240015] [21:43:59.240042] [21:43:59.239955] [21:43:59.240074] [21:43:59.240070] [21:43:59.240097] [21:43:59.240065] [21:43:59.240124] [21:43:59.240120] [21:43:59.240147] [21:43:59.239275] [21:43:59.240192] [21:43:59.240188] [21:43:59.240215] [21:43:59.240183] [21:43:59.240243] [21:43:59.240238] [21:43:59.240266] [21:43:59.240179] [21:43:59.240298] [21:43:59.240294] [21:43:59.240321] [21:43:59.240289] [21:43:59.240349] [21:43:59.240345] [21:43:59.240372] [21:43:59.240175] [21:43:59.240409] [21:43:59.240405] [21:43:59.240432] [21:43:59.240400] [21:43:59.240459] [21:43:59.240455] [21:43:59.240482] [21:43:59.240396] [21:43:59.240521] [21:43:59.240517] [21:43:59.240544] [21:43:59.240512] [21:43:59.240572] [21:43:59.240568] [21:43:59.240595] [21:43:59.240170] [21:43:59.240636] [21:43:59.240632] [21:43:59.240659] [21:43:59.240628] [21:43:59.240687] [21:43:59.240682] [21:43:59.240709] [21:43:59.240623] [21:43:59.240743] [21:43:59.240739] [21:43:59.240766] [21:43:59.240734] [21:43:59.240794] [21:43:59.240790] [21:43:59.240817] [21:43:59.240619] [21:43:59.240853] [21:43:59.240849] [21:43:59.240877] [21:43:59.240845] [21:43:59.240904] [21:43:59.240900] [21:43:59.240927] [21:43:59.240840] [21:43:59.240960] [21:43:59.240955] [21:43:59.240983] [21:43:59.240951] [21:43:59.241010] [21:43:59.241006] [21:43:59.241033] model.diffusion_model.res_blocks.0.in_layers.0.bias torch.Size([1536])\n",
      "[21:43:59.304808] [21:43:59.304804] [21:43:59.304845] [21:43:59.304799] [21:43:59.304877] [21:43:59.304872] [21:43:59.304902] [21:43:59.304794] [21:43:59.304936] [21:43:59.304931] [21:43:59.304960] [21:43:59.304927] [21:43:59.304989] [21:43:59.304984] [21:43:59.305014] [21:43:59.304789] [21:43:59.305052] [21:43:59.305047] [21:43:59.305076] [21:43:59.305043] [21:43:59.305105] [21:43:59.305100] [21:43:59.305129] [21:43:59.305038] [21:43:59.305162] [21:43:59.305157] [21:43:59.305185] [21:43:59.305153] [21:43:59.305214] [21:43:59.305209] [21:43:59.305237] [21:43:59.304782] [21:43:59.305279] [21:43:59.305275] [21:43:59.305303] [21:43:59.305270] [21:43:59.305333] [21:43:59.305328] [21:43:59.305359] [21:43:59.305266] [21:43:59.305393] [21:43:59.305388] [21:43:59.305417] [21:43:59.305383] [21:43:59.305447] [21:43:59.305442] [21:43:59.305471] [21:43:59.305261] [21:43:59.305517] [21:43:59.305512] [21:43:59.305541] [21:43:59.305507] [21:43:59.305570] [21:43:59.305565] [21:43:59.305595] [21:43:59.305495] [21:43:59.305628] [21:43:59.305623] [21:43:59.305653] [21:43:59.305619] [21:43:59.305681] [21:43:59.305677] [21:43:59.305705] [21:43:59.304766] [21:43:59.305753] [21:43:59.305748] [21:43:59.305778] [21:43:59.305744] [21:43:59.305806] [21:43:59.305802] [21:43:59.305830] [21:43:59.305739] [21:43:59.305863] [21:43:59.305859] [21:43:59.305887] [21:43:59.305854] [21:43:59.305916] [21:43:59.305911] [21:43:59.305940] [21:43:59.305735] [21:43:59.305977] [21:43:59.305973] [21:43:59.306001] [21:43:59.305969] [21:43:59.306030] [21:43:59.306026] [21:43:59.306054] [21:43:59.305964] [21:43:59.306088] [21:43:59.306083] [21:43:59.306112] [21:43:59.306078] [21:43:59.306140] [21:43:59.306136] [21:43:59.306164] [21:43:59.305730] [21:43:59.306208] [21:43:59.306204] [21:43:59.306232] [21:43:59.306199] [21:43:59.306261] [21:43:59.306256] [21:43:59.306284] [21:43:59.306195] [21:43:59.306317] [21:43:59.306312] [21:43:59.306340] [21:43:59.306308] [21:43:59.306368] [21:43:59.306364] [21:43:59.306392] [21:43:59.306190] [21:43:59.306430] [21:43:59.306425] [21:43:59.306453] [21:43:59.306421] [21:43:59.306481] [21:43:59.306476] [21:43:59.306511] [21:43:59.306416] [21:43:59.306545] [21:43:59.306540] [21:43:59.306568] [21:43:59.306535] [21:43:59.306597] [21:43:59.306592] [21:43:59.306621] prefixes: [('model_ema', 132), ('model', 130), ('betas', 1), ('alphas_cumprod', 1), ('alphas_cumprod_prev', 1), ('sqrt_alphas_cumprod', 1), ('sqrt_one_minus_alphas_cumprod', 1), ('log_one_minus_alphas_cumprod', 1), ('sqrt_recip_alphas_cumprod', 1), ('sqrt_recipm1_alphas_cumprod', 1)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "ckpt_path = \"pretrained_enc_ckpts/mocov3/rdm-mocov3vitb.pth\"  # update if needed\n",
    "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "print(\"ckpt keys:\", list(ckpt.keys()) if isinstance(ckpt, dict) else type(ckpt))\n",
    "state = ckpt[\"model\"] if isinstance(ckpt, dict) and \"model\" in ckpt else (\n",
    "    ckpt[\"state_dict\"] if isinstance(ckpt, dict) and \"state_dict\" in ckpt else ckpt\n",
    ")\n",
    "\n",
    "print(\"state keys:\", len(state))\n",
    "print(\"first 20 keys:\")\n",
    "for k in list(state.keys())[:20]:\n",
    "    v = state[k]\n",
    "    print(k, getattr(v, \"shape\", None))\n",
    "\n",
    "# prefix histogram (helps spot base_encoder/module prefixes)\n",
    "from collections import Counter\n",
    "prefixes = Counter([k.split(\".\")[0] for k in state.keys()])\n",
    "print(\"prefixes:\", prefixes.most_common(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "694217bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:45:14.844908] [21:45:14.844902] [21:45:14.850422] [21:45:14.844897] [21:45:14.850467] [21:45:14.850462] [21:45:14.850493] [21:45:14.844886] [21:45:14.850540] [21:45:14.850535] [21:45:14.850565] [21:45:14.850526] [21:45:14.850594] [21:45:14.850589] [21:45:14.850619] [21:45:14.844875] [21:45:14.850660] [21:45:14.850656] [21:45:14.850685] [21:45:14.850651] [21:45:14.850714] [21:45:14.850710] [21:45:14.850742] [21:45:14.850646] [21:45:14.850776] [21:45:14.850771] [21:45:14.850800] [21:45:14.850766] [21:45:14.850832] [21:45:14.850827] [21:45:14.850856] [21:45:14.844867] [21:45:14.850899] [21:45:14.850894] [21:45:14.850923] [21:45:14.850890] [21:45:14.850952] [21:45:14.850948] [21:45:14.850976] [21:45:14.850885] [21:45:14.851012] [21:45:14.851008] [21:45:14.851036] [21:45:14.851000] [21:45:14.851068] [21:45:14.851063] [21:45:14.851091] [21:45:14.850881] [21:45:14.851128] [21:45:14.851124] [21:45:14.851155] [21:45:14.851119] [21:45:14.851186] [21:45:14.851182] [21:45:14.851215] [21:45:14.851115] [21:45:14.851249] [21:45:14.851244] [21:45:14.851272] [21:45:14.851240] [21:45:14.851300] [21:45:14.851296] [21:45:14.851327] [21:45:14.843768] [21:45:14.851378] [21:45:14.851370] [21:45:14.851403] [21:45:14.851366] [21:45:14.851432] [21:45:14.851427] [21:45:14.851456] [21:45:14.851362] [21:45:14.851489] [21:45:14.851484] [21:45:14.851523] [21:45:14.851480] [21:45:14.851552] [21:45:14.851548] [21:45:14.851577] [21:45:14.851357] [21:45:14.851614] [21:45:14.851610] [21:45:14.851638] [21:45:14.851605] [21:45:14.851666] [21:45:14.851661] [21:45:14.851692] [21:45:14.851601] [21:45:14.851725] [21:45:14.851721] [21:45:14.851749] [21:45:14.851716] [21:45:14.851779] [21:45:14.851775] [21:45:14.851803] [21:45:14.851352] [21:45:14.851848] [21:45:14.851844] [21:45:14.851872] [21:45:14.851836] [21:45:14.851903] [21:45:14.851899] [21:45:14.851927] [21:45:14.851832] [21:45:14.851960] [21:45:14.851955] [21:45:14.851986] [21:45:14.851951] [21:45:14.852014] [21:45:14.852010] [21:45:14.852041] [21:45:14.851827] [21:45:14.852078] [21:45:14.852074] [21:45:14.852102] [21:45:14.852069] [21:45:14.852130] [21:45:14.852125] [21:45:14.852153] [21:45:14.852065] [21:45:14.852186] [21:45:14.852181] [21:45:14.852209] [21:45:14.852177] [21:45:14.852237] [21:45:14.852232] [21:45:14.852260] model keys: 166\n",
      "[21:45:14.852397] [21:45:14.852391] [21:45:14.852429] [21:45:14.852386] [21:45:14.852464] [21:45:14.852459] [21:45:14.852490] [21:45:14.852382] [21:45:14.852620] [21:45:14.852607] [21:45:14.852646] [21:45:14.852601] [21:45:14.852676] [21:45:14.852671] [21:45:14.852700] [21:45:14.852377] [21:45:14.852738] [21:45:14.852733] [21:45:14.852778] [21:45:14.852729] [21:45:14.852822] [21:45:14.852815] [21:45:14.852857] [21:45:14.852724] [21:45:14.852891] [21:45:14.852886] [21:45:14.852915] [21:45:14.852882] [21:45:14.852943] [21:45:14.852939] [21:45:14.852967] [21:45:14.852372] [21:45:14.853013] [21:45:14.853008] [21:45:14.853037] [21:45:14.853004] [21:45:14.853069] [21:45:14.853064] [21:45:14.853093] [21:45:14.852999] [21:45:14.853129] [21:45:14.853124] [21:45:14.853153] [21:45:14.853119] [21:45:14.853188] [21:45:14.853183] [21:45:14.853214] [21:45:14.852991] [21:45:14.853251] [21:45:14.853247] [21:45:14.853276] [21:45:14.853243] [21:45:14.853305] [21:45:14.853300] [21:45:14.853329] [21:45:14.853238] [21:45:14.853363] [21:45:14.853358] [21:45:14.853387] [21:45:14.853354] [21:45:14.853416] [21:45:14.853411] [21:45:14.853444] [21:45:14.852365] [21:45:14.853491] [21:45:14.853487] [21:45:14.853527] [21:45:14.853483] [21:45:14.853562] [21:45:14.853552] [21:45:14.853586] [21:45:14.853478] [21:45:14.853620] [21:45:14.853616] [21:45:14.853644] [21:45:14.853611] [21:45:14.853680] [21:45:14.853672] [21:45:14.853709] [21:45:14.853474] [21:45:14.853747] [21:45:14.853742] [21:45:14.853774] [21:45:14.853738] [21:45:14.853803] [21:45:14.853798] [21:45:14.853827] [21:45:14.853733] [21:45:14.853861] [21:45:14.853856] [21:45:14.853885] [21:45:14.853851] [21:45:14.853913] [21:45:14.853909] [21:45:14.853937] [21:45:14.853469] [21:45:14.853982] [21:45:14.853978] [21:45:14.854009] [21:45:14.853974] [21:45:14.854037] [21:45:14.854033] [21:45:14.854061] [21:45:14.853969] [21:45:14.854094] [21:45:14.854090] [21:45:14.854118] [21:45:14.854085] [21:45:14.854147] [21:45:14.854142] [21:45:14.854171] [21:45:14.853964] [21:45:14.854215] [21:45:14.854211] [21:45:14.854240] [21:45:14.854206] [21:45:14.854271] [21:45:14.854266] [21:45:14.854297] [21:45:14.854198] [21:45:14.854333] [21:45:14.854329] [21:45:14.854358] [21:45:14.854321] [21:45:14.854386] [21:45:14.854381] [21:45:14.854409] first 20 model keys:\n",
      "[21:45:14.856844] [21:45:14.856837] [21:45:14.856884] [21:45:14.856833] [21:45:14.856916] [21:45:14.856911] [21:45:14.856941] [21:45:14.856828] [21:45:14.856978] [21:45:14.856974] [21:45:14.857004] [21:45:14.856969] [21:45:14.857037] [21:45:14.857032] [21:45:14.857063] [21:45:14.856824] [21:45:14.857103] [21:45:14.857096] [21:45:14.857127] [21:45:14.857091] [21:45:14.857155] [21:45:14.857150] [21:45:14.857179] [21:45:14.857087] [21:45:14.857215] [21:45:14.857210] [21:45:14.857238] [21:45:14.857206] [21:45:14.857266] [21:45:14.857262] [21:45:14.857292] [21:45:14.856818] [21:45:14.857335] [21:45:14.857331] [21:45:14.857360] [21:45:14.857327] [21:45:14.857392] [21:45:14.857387] [21:45:14.857416] [21:45:14.857322] [21:45:14.857453] [21:45:14.857448] [21:45:14.857477] [21:45:14.857443] [21:45:14.857521] [21:45:14.857515] [21:45:14.857546] [21:45:14.857317] [21:45:14.857587] [21:45:14.857582] [21:45:14.857611] [21:45:14.857578] [21:45:14.857640] [21:45:14.857635] [21:45:14.857663] [21:45:14.857573] [21:45:14.857699] [21:45:14.857694] [21:45:14.857722] [21:45:14.857690] [21:45:14.857753] [21:45:14.857746] [21:45:14.857778] [21:45:14.856808] [21:45:14.857825] [21:45:14.857820] [21:45:14.857849] [21:45:14.857816] [21:45:14.857878] [21:45:14.857873] [21:45:14.857904] [21:45:14.857812] [21:45:14.857938] [21:45:14.857933] [21:45:14.857961] [21:45:14.857929] [21:45:14.857989] [21:45:14.857985] [21:45:14.858013] [21:45:14.857807] [21:45:14.858057] [21:45:14.858053] [21:45:14.858082] [21:45:14.858044] [21:45:14.858110] [21:45:14.858106] [21:45:14.858134] [21:45:14.858037] [21:45:14.858167] [21:45:14.858162] [21:45:14.858190] [21:45:14.858158] [21:45:14.858219] [21:45:14.858214] [21:45:14.858242] [21:45:14.857803] [21:45:14.858287] [21:45:14.858283] [21:45:14.858310] [21:45:14.858278] [21:45:14.858341] [21:45:14.858337] [21:45:14.858364] [21:45:14.858274] [21:45:14.858400] [21:45:14.858396] [21:45:14.858426] [21:45:14.858391] [21:45:14.858455] [21:45:14.858450] [21:45:14.858478] [21:45:14.858269] [21:45:14.858524] [21:45:14.858519] [21:45:14.858548] [21:45:14.858514] [21:45:14.858577] [21:45:14.858572] [21:45:14.858600] [21:45:14.858509] [21:45:14.858632] [21:45:14.858628] [21:45:14.858660] [21:45:14.858623] [21:45:14.858688] [21:45:14.858683] [21:45:14.858711] cls_token torch.Size([1, 1, 768])\n",
      "[21:45:14.859739] [21:45:14.859734] [21:45:14.859768] [21:45:14.859726] [21:45:14.859797] [21:45:14.859793] [21:45:14.859824] [21:45:14.859722] [21:45:14.859859] [21:45:14.859855] [21:45:14.859887] [21:45:14.859850] [21:45:14.859916] [21:45:14.859911] [21:45:14.859941] [21:45:14.859717] [21:45:14.859978] [21:45:14.859974] [21:45:14.860002] [21:45:14.859969] [21:45:14.860031] [21:45:14.860026] [21:45:14.860055] [21:45:14.859965] [21:45:14.860092] [21:45:14.860087] [21:45:14.860116] [21:45:14.860082] [21:45:14.860144] [21:45:14.860139] [21:45:14.860168] [21:45:14.859712] [21:45:14.860213] [21:45:14.860209] [21:45:14.860240] [21:45:14.860204] [21:45:14.860268] [21:45:14.860264] [21:45:14.860292] [21:45:14.860200] [21:45:14.860325] [21:45:14.860321] [21:45:14.860349] [21:45:14.860316] [21:45:14.860378] [21:45:14.860373] [21:45:14.860401] [21:45:14.860195] [21:45:14.860439] [21:45:14.860434] [21:45:14.860466] [21:45:14.860430] [21:45:14.860499] [21:45:14.860494] [21:45:14.860532] [21:45:14.860425] [21:45:14.860565] [21:45:14.860560] [21:45:14.860588] [21:45:14.860556] [21:45:14.860617] [21:45:14.860612] [21:45:14.860640] [21:45:14.859705] [21:45:14.860686] [21:45:14.860682] [21:45:14.860714] [21:45:14.860677] [21:45:14.860743] [21:45:14.860738] [21:45:14.860766] [21:45:14.860673] [21:45:14.860802] [21:45:14.860797] [21:45:14.860826] [21:45:14.860793] [21:45:14.860854] [21:45:14.860850] [21:45:14.860878] [21:45:14.860669] [21:45:14.860916] [21:45:14.860911] [21:45:14.860940] [21:45:14.860907] [21:45:14.860968] [21:45:14.860964] [21:45:14.860992] [21:45:14.860902] [21:45:14.861025] [21:45:14.861020] [21:45:14.861048] [21:45:14.861016] [21:45:14.861076] [21:45:14.861072] [21:45:14.861100] [21:45:14.860664] [21:45:14.861145] [21:45:14.861137] [21:45:14.861169] [21:45:14.861133] [21:45:14.861197] [21:45:14.861193] [21:45:14.861227] [21:45:14.861128] [21:45:14.861266] [21:45:14.861261] [21:45:14.861293] [21:45:14.861254] [21:45:14.861321] [21:45:14.861317] [21:45:14.861348] [21:45:14.861124] [21:45:14.861385] [21:45:14.861381] [21:45:14.861409] [21:45:14.861376] [21:45:14.861437] [21:45:14.861433] [21:45:14.861461] [21:45:14.861372] [21:45:14.861494] [21:45:14.861489] [21:45:14.861526] [21:45:14.861485] [21:45:14.861558] [21:45:14.861553] [21:45:14.861581] pos_embed torch.Size([1, 197, 768])\n",
      "[21:45:14.861649] [21:45:14.861644] [21:45:14.861674] [21:45:14.861640] [21:45:14.861707] [21:45:14.861702] [21:45:14.861731] [21:45:14.861635] [21:45:14.861767] [21:45:14.861763] [21:45:14.861792] [21:45:14.861758] [21:45:14.861821] [21:45:14.861816] [21:45:14.861845] [21:45:14.861631] [21:45:14.861887] [21:45:14.861882] [21:45:14.861910] [21:45:14.861878] [21:45:14.861939] [21:45:14.861935] [21:45:14.861966] [21:45:14.861873] [21:45:14.862006] [21:45:14.861996] [21:45:14.862031] [21:45:14.861991] [21:45:14.862059] [21:45:14.862055] [21:45:14.862083] [21:45:14.861627] [21:45:14.862125] [21:45:14.862121] [21:45:14.862152] [21:45:14.862116] [21:45:14.862180] [21:45:14.862176] [21:45:14.862207] [21:45:14.862112] [21:45:14.862240] [21:45:14.862236] [21:45:14.862267] [21:45:14.862231] [21:45:14.862295] [21:45:14.862291] [21:45:14.862322] [21:45:14.862107] [21:45:14.862359] [21:45:14.862355] [21:45:14.862383] [21:45:14.862350] [21:45:14.862412] [21:45:14.862407] [21:45:14.862435] [21:45:14.862346] [21:45:14.862468] [21:45:14.862464] [21:45:14.862492] [21:45:14.862459] [21:45:14.862528] [21:45:14.862523] [21:45:14.862553] [21:45:14.861622] [21:45:14.862599] [21:45:14.862594] [21:45:14.862623] [21:45:14.862590] [21:45:14.862652] [21:45:14.862647] [21:45:14.862680] [21:45:14.862586] [21:45:14.862716] [21:45:14.862711] [21:45:14.862743] [21:45:14.862707] [21:45:14.862772] [21:45:14.862768] [21:45:14.862796] [21:45:14.862581] [21:45:14.862834] [21:45:14.862829] [21:45:14.862861] [21:45:14.862825] [21:45:14.862890] [21:45:14.862885] [21:45:14.862913] [21:45:14.862820] [21:45:14.862946] [21:45:14.862942] [21:45:14.862970] [21:45:14.862937] [21:45:14.862998] [21:45:14.862994] [21:45:14.863022] [21:45:14.862577] [21:45:14.863069] [21:45:14.863065] [21:45:14.863093] [21:45:14.863061] [21:45:14.863124] [21:45:14.863119] [21:45:14.863148] [21:45:14.863054] [21:45:14.863181] [21:45:14.863177] [21:45:14.863205] [21:45:14.863172] [21:45:14.863233] [21:45:14.863229] [21:45:14.863257] [21:45:14.863049] [21:45:14.863295] [21:45:14.863290] [21:45:14.863321] [21:45:14.863286] [21:45:14.863349] [21:45:14.863345] [21:45:14.863373] [21:45:14.863281] [21:45:14.863410] [21:45:14.863404] [21:45:14.863439] [21:45:14.863399] [21:45:14.863468] [21:45:14.863463] [21:45:14.863494] patch_embed.proj.weight torch.Size([768, 3, 16, 16])\n",
      "[21:45:14.863569] [21:45:14.863565] [21:45:14.863594] [21:45:14.863560] [21:45:14.863627] [21:45:14.863622] [21:45:14.863650] [21:45:14.863556] [21:45:14.863686] [21:45:14.863682] [21:45:14.863710] [21:45:14.863677] [21:45:14.863742] [21:45:14.863737] [21:45:14.863768] [21:45:14.863552] [21:45:14.863805] [21:45:14.863801] [21:45:14.863830] [21:45:14.863796] [21:45:14.863859] [21:45:14.863854] [21:45:14.863883] [21:45:14.863792] [21:45:14.863916] [21:45:14.863912] [21:45:14.863940] [21:45:14.863907] [21:45:14.863968] [21:45:14.863964] [21:45:14.863992] [21:45:14.863547] [21:45:14.864033] [21:45:14.864029] [21:45:14.864057] [21:45:14.864025] [21:45:14.864086] [21:45:14.864081] [21:45:14.864109] [21:45:14.864020] [21:45:14.864145] [21:45:14.864141] [21:45:14.864169] [21:45:14.864136] [21:45:14.864200] [21:45:14.864195] [21:45:14.864232] [21:45:14.864016] [21:45:14.864278] [21:45:14.864273] [21:45:14.864302] [21:45:14.864269] [21:45:14.864330] [21:45:14.864326] [21:45:14.864354] [21:45:14.864264] [21:45:14.864390] [21:45:14.864386] [21:45:14.864416] [21:45:14.864381] [21:45:14.864445] [21:45:14.864441] [21:45:14.864469] [21:45:14.863542] [21:45:14.864525] [21:45:14.864521] [21:45:14.864550] [21:45:14.864517] [21:45:14.864579] [21:45:14.864574] [21:45:14.864603] [21:45:14.864512] [21:45:14.864642] [21:45:14.864634] [21:45:14.864668] [21:45:14.864630] [21:45:14.864700] [21:45:14.864695] [21:45:14.864728] [21:45:14.864498] [21:45:14.864771] [21:45:14.864764] [21:45:14.864795] [21:45:14.864760] [21:45:14.864823] [21:45:14.864819] [21:45:14.864847] [21:45:14.864753] [21:45:14.864883] [21:45:14.864878] [21:45:14.864907] [21:45:14.864874] [21:45:14.864935] [21:45:14.864931] [21:45:14.864959] [21:45:14.864493] [21:45:14.865011] [21:45:14.865004] [21:45:14.865038] [21:45:14.864997] [21:45:14.865069] [21:45:14.865064] [21:45:14.865095] [21:45:14.864993] [21:45:14.865128] [21:45:14.865124] [21:45:14.865152] [21:45:14.865119] [21:45:14.865184] [21:45:14.865179] [21:45:14.865207] [21:45:14.864986] [21:45:14.865245] [21:45:14.865240] [21:45:14.865271] [21:45:14.865236] [21:45:14.865300] [21:45:14.865295] [21:45:14.865323] [21:45:14.865231] [21:45:14.865356] [21:45:14.865352] [21:45:14.865380] [21:45:14.865347] [21:45:14.865408] [21:45:14.865404] [21:45:14.865432] patch_embed.proj.bias torch.Size([768])\n",
      "[21:45:14.865497] [21:45:14.865493] [21:45:14.865527] [21:45:14.865488] [21:45:14.865556] [21:45:14.865552] [21:45:14.865580] [21:45:14.865484] [21:45:14.865613] [21:45:14.865608] [21:45:14.865636] [21:45:14.865604] [21:45:14.865667] [21:45:14.865663] [21:45:14.865691] [21:45:14.865480] [21:45:14.865728] [21:45:14.865723] [21:45:14.865756] [21:45:14.865719] [21:45:14.865788] [21:45:14.865784] [21:45:14.865812] [21:45:14.865714] [21:45:14.865845] [21:45:14.865841] [21:45:14.865869] [21:45:14.865836] [21:45:14.865903] [21:45:14.865898] [21:45:14.865930] [21:45:14.865475] [21:45:14.865975] [21:45:14.865971] [21:45:14.865999] [21:45:14.865966] [21:45:14.866031] [21:45:14.866026] [21:45:14.866055] [21:45:14.865962] [21:45:14.866087] [21:45:14.866083] [21:45:14.866115] [21:45:14.866078] [21:45:14.866147] [21:45:14.866139] [21:45:14.866173] [21:45:14.865957] [21:45:14.866213] [21:45:14.866208] [21:45:14.866237] [21:45:14.866204] [21:45:14.866273] [21:45:14.866268] [21:45:14.866297] [21:45:14.866200] [21:45:14.866332] [21:45:14.866328] [21:45:14.866356] [21:45:14.866323] [21:45:14.866384] [21:45:14.866380] [21:45:14.866408] [21:45:14.865471] [21:45:14.866454] [21:45:14.866449] [21:45:14.866477] [21:45:14.866445] [21:45:14.866519] [21:45:14.866514] [21:45:14.866543] [21:45:14.866441] [21:45:14.866576] [21:45:14.866572] [21:45:14.866606] [21:45:14.866567] [21:45:14.866640] [21:45:14.866636] [21:45:14.866665] [21:45:14.866436] [21:45:14.866705] [21:45:14.866698] [21:45:14.866729] [21:45:14.866693] [21:45:14.866758] [21:45:14.866753] [21:45:14.866782] [21:45:14.866689] [21:45:14.866815] [21:45:14.866811] [21:45:14.866839] [21:45:14.866806] [21:45:14.866868] [21:45:14.866863] [21:45:14.866895] [21:45:14.866432] [21:45:14.866937] [21:45:14.866933] [21:45:14.866963] [21:45:14.866928] [21:45:14.866992] [21:45:14.866987] [21:45:14.867015] [21:45:14.866924] [21:45:14.867048] [21:45:14.867044] [21:45:14.867071] [21:45:14.867039] [21:45:14.867100] [21:45:14.867096] [21:45:14.867126] [21:45:14.866919] [21:45:14.867164] [21:45:14.867159] [21:45:14.867188] [21:45:14.867155] [21:45:14.867222] [21:45:14.867214] [21:45:14.867246] [21:45:14.867151] [21:45:14.867284] [21:45:14.867275] [21:45:14.867311] [21:45:14.867271] [21:45:14.867341] [21:45:14.867336] [21:45:14.867365] blocks.0.norm1.weight torch.Size([768])\n",
      "[21:45:14.867430] [21:45:14.867426] [21:45:14.867457] [21:45:14.867421] [21:45:14.867490] [21:45:14.867485] [21:45:14.867527] [21:45:14.867417] [21:45:14.867561] [21:45:14.867556] [21:45:14.867586] [21:45:14.867551] [21:45:14.867618] [21:45:14.867614] [21:45:14.867642] [21:45:14.867413] [21:45:14.867680] [21:45:14.867675] [21:45:14.867703] [21:45:14.867671] [21:45:14.867732] [21:45:14.867727] [21:45:14.867755] [21:45:14.867666] [21:45:14.867788] [21:45:14.867784] [21:45:14.867812] [21:45:14.867779] [21:45:14.867840] [21:45:14.867836] [21:45:14.867864] [21:45:14.867408] [21:45:14.867906] [21:45:14.867901] [21:45:14.867929] [21:45:14.867897] [21:45:14.867964] [21:45:14.867957] [21:45:14.867997] [21:45:14.867893] [21:45:14.868038] [21:45:14.868030] [21:45:14.868068] [21:45:14.868025] [21:45:14.868098] [21:45:14.868093] [21:45:14.868122] [21:45:14.867888] [21:45:14.868163] [21:45:14.868158] [21:45:14.868187] [21:45:14.868150] [21:45:14.868216] [21:45:14.868211] [21:45:14.868240] [21:45:14.868146] [21:45:14.868274] [21:45:14.868269] [21:45:14.868301] [21:45:14.868265] [21:45:14.868334] [21:45:14.868329] [21:45:14.868358] [21:45:14.867403] [21:45:14.868408] [21:45:14.868404] [21:45:14.868432] [21:45:14.868400] [21:45:14.868461] [21:45:14.868456] [21:45:14.868489] [21:45:14.868395] [21:45:14.868533] [21:45:14.868528] [21:45:14.868557] [21:45:14.868523] [21:45:14.868586] [21:45:14.868581] [21:45:14.868610] [21:45:14.868391] [21:45:14.868647] [21:45:14.868643] [21:45:14.868676] [21:45:14.868639] [21:45:14.868705] [21:45:14.868700] [21:45:14.868728] [21:45:14.868634] [21:45:14.868765] [21:45:14.868761] [21:45:14.868794] [21:45:14.868753] [21:45:14.868831] [21:45:14.868827] [21:45:14.868856] [21:45:14.868386] [21:45:14.868901] [21:45:14.868897] [21:45:14.868925] [21:45:14.868893] [21:45:14.868961] [21:45:14.868954] [21:45:14.868986] [21:45:14.868888] [21:45:14.869023] [21:45:14.869018] [21:45:14.869047] [21:45:14.869010] [21:45:14.869082] [21:45:14.869074] [21:45:14.869106] [21:45:14.868880] [21:45:14.869144] [21:45:14.869139] [21:45:14.869167] [21:45:14.869135] [21:45:14.869196] [21:45:14.869191] [21:45:14.869219] [21:45:14.869130] [21:45:14.869253] [21:45:14.869248] [21:45:14.869277] [21:45:14.869244] [21:45:14.869309] [21:45:14.869304] [21:45:14.869332] blocks.0.norm1.bias torch.Size([768])\n",
      "[21:45:14.869400] [21:45:14.869395] [21:45:14.869429] [21:45:14.869391] [21:45:14.869457] [21:45:14.869453] [21:45:14.869484] [21:45:14.869384] [21:45:14.869524] [21:45:14.869519] [21:45:14.869551] [21:45:14.869515] [21:45:14.869582] [21:45:14.869578] [21:45:14.869606] [21:45:14.869380] [21:45:14.869646] [21:45:14.869642] [21:45:14.869670] [21:45:14.869638] [21:45:14.869698] [21:45:14.869694] [21:45:14.869725] [21:45:14.869633] [21:45:14.869758] [21:45:14.869753] [21:45:14.869785] [21:45:14.869749] [21:45:14.869813] [21:45:14.869809] [21:45:14.869837] [21:45:14.869375] [21:45:14.869879] [21:45:14.869874] [21:45:14.869902] [21:45:14.869870] [21:45:14.869931] [21:45:14.869926] [21:45:14.869955] [21:45:14.869866] [21:45:14.869990] [21:45:14.869986] [21:45:14.870015] [21:45:14.869981] [21:45:14.870043] [21:45:14.870038] [21:45:14.870069] [21:45:14.869861] [21:45:14.870114] [21:45:14.870109] [21:45:14.870138] [21:45:14.870102] [21:45:14.870169] [21:45:14.870164] [21:45:14.870192] [21:45:14.870098] [21:45:14.870228] [21:45:14.870223] [21:45:14.870252] [21:45:14.870219] [21:45:14.870283] [21:45:14.870279] [21:45:14.870308] [21:45:14.869367] [21:45:14.870360] [21:45:14.870356] [21:45:14.870384] [21:45:14.870351] [21:45:14.870416] [21:45:14.870408] [21:45:14.870439] [21:45:14.870347] [21:45:14.870475] [21:45:14.870471] [21:45:14.870499] [21:45:14.870466] [21:45:14.870532] [21:45:14.870528] [21:45:14.870556] [21:45:14.870342] [21:45:14.870592] [21:45:14.870588] [21:45:14.870616] [21:45:14.870584] [21:45:14.870647] [21:45:14.870642] [21:45:14.870670] [21:45:14.870579] [21:45:14.870703] [21:45:14.870699] [21:45:14.870726] [21:45:14.870694] [21:45:14.870755] [21:45:14.870750] [21:45:14.870781] [21:45:14.870338] [21:45:14.870822] [21:45:14.870818] [21:45:14.870849] [21:45:14.870814] [21:45:14.870882] [21:45:14.870875] [21:45:14.870909] [21:45:14.870810] [21:45:14.870944] [21:45:14.870937] [21:45:14.870968] [21:45:14.870933] [21:45:14.870997] [21:45:14.870992] [21:45:14.871021] [21:45:14.870805] [21:45:14.871061] [21:45:14.871054] [21:45:14.871085] [21:45:14.871049] [21:45:14.871113] [21:45:14.871109] [21:45:14.871137] [21:45:14.871045] [21:45:14.871173] [21:45:14.871168] [21:45:14.871196] [21:45:14.871164] [21:45:14.871227] [21:45:14.871223] [21:45:14.871250] blocks.0.attn.qkv.weight torch.Size([2304, 768])\n",
      "[21:45:14.871312] [21:45:14.871307] [21:45:14.871336] [21:45:14.871303] [21:45:14.871367] [21:45:14.871363] [21:45:14.871391] [21:45:14.871299] [21:45:14.871424] [21:45:14.871420] [21:45:14.871451] [21:45:14.871415] [21:45:14.871480] [21:45:14.871475] [21:45:14.871508] [21:45:14.871294] [21:45:14.871547] [21:45:14.871542] [21:45:14.871571] [21:45:14.871538] [21:45:14.871599] [21:45:14.871595] [21:45:14.871623] [21:45:14.871533] [21:45:14.871659] [21:45:14.871654] [21:45:14.871683] [21:45:14.871650] [21:45:14.871712] [21:45:14.871707] [21:45:14.871741] [21:45:14.871290] [21:45:14.871788] [21:45:14.871784] [21:45:14.871817] [21:45:14.871779] [21:45:14.871846] [21:45:14.871842] [21:45:14.871870] [21:45:14.871775] [21:45:14.871903] [21:45:14.871899] [21:45:14.871930] [21:45:14.871894] [21:45:14.871959] [21:45:14.871954] [21:45:14.871985] [21:45:14.871770] [21:45:14.872022] [21:45:14.872018] [21:45:14.872049] [21:45:14.872014] [21:45:14.872078] [21:45:14.872073] [21:45:14.872102] [21:45:14.872009] [21:45:14.872135] [21:45:14.872131] [21:45:14.872159] [21:45:14.872126] [21:45:14.872187] [21:45:14.872183] [21:45:14.872211] [21:45:14.871285] [21:45:14.872260] [21:45:14.872255] [21:45:14.872283] [21:45:14.872251] [21:45:14.872315] [21:45:14.872310] [21:45:14.872339] [21:45:14.872247] [21:45:14.872374] [21:45:14.872367] [21:45:14.872406] [21:45:14.872363] [21:45:14.872435] [21:45:14.872431] [21:45:14.872461] [21:45:14.872243] [21:45:14.872499] [21:45:14.872494] [21:45:14.872529] [21:45:14.872490] [21:45:14.872560] [21:45:14.872553] [21:45:14.872588] [21:45:14.872485] [21:45:14.872624] [21:45:14.872620] [21:45:14.872648] [21:45:14.872615] [21:45:14.872677] [21:45:14.872672] [21:45:14.872700] [21:45:14.872238] [21:45:14.872745] [21:45:14.872741] [21:45:14.872769] [21:45:14.872736] [21:45:14.872797] [21:45:14.872792] [21:45:14.872823] [21:45:14.872732] [21:45:14.872856] [21:45:14.872852] [21:45:14.872883] [21:45:14.872847] [21:45:14.872911] [21:45:14.872907] [21:45:14.872935] [21:45:14.872727] [21:45:14.872973] [21:45:14.872969] [21:45:14.872997] [21:45:14.872964] [21:45:14.873025] [21:45:14.873021] [21:45:14.873049] [21:45:14.872960] [21:45:14.873082] [21:45:14.873078] [21:45:14.873106] [21:45:14.873073] [21:45:14.873134] [21:45:14.873130] [21:45:14.873158] blocks.0.attn.qkv.bias torch.Size([2304])\n",
      "[21:45:14.873220] [21:45:14.873215] [21:45:14.873244] [21:45:14.873211] [21:45:14.873276] [21:45:14.873271] [21:45:14.873300] [21:45:14.873207] [21:45:14.873334] [21:45:14.873329] [21:45:14.873358] [21:45:14.873325] [21:45:14.873386] [21:45:14.873382] [21:45:14.873410] [21:45:14.873202] [21:45:14.873448] [21:45:14.873443] [21:45:14.873471] [21:45:14.873439] [21:45:14.873506] [21:45:14.873496] [21:45:14.873531] [21:45:14.873435] [21:45:14.873569] [21:45:14.873564] [21:45:14.873594] [21:45:14.873559] [21:45:14.873623] [21:45:14.873618] [21:45:14.873647] [21:45:14.873198] [21:45:14.873689] [21:45:14.873684] [21:45:14.873713] [21:45:14.873680] [21:45:14.873742] [21:45:14.873737] [21:45:14.873766] [21:45:14.873676] [21:45:14.873799] [21:45:14.873794] [21:45:14.873823] [21:45:14.873790] [21:45:14.873851] [21:45:14.873847] [21:45:14.873875] [21:45:14.873671] [21:45:14.873913] [21:45:14.873909] [21:45:14.873937] [21:45:14.873904] [21:45:14.873965] [21:45:14.873961] [21:45:14.873989] [21:45:14.873900] [21:45:14.874025] [21:45:14.874018] [21:45:14.874050] [21:45:14.874013] [21:45:14.874078] [21:45:14.874074] [21:45:14.874102] [21:45:14.873193] [21:45:14.874149] [21:45:14.874144] [21:45:14.874172] [21:45:14.874140] [21:45:14.874201] [21:45:14.874196] [21:45:14.874225] [21:45:14.874136] [21:45:14.874258] [21:45:14.874254] [21:45:14.874282] [21:45:14.874249] [21:45:14.874310] [21:45:14.874306] [21:45:14.874334] [21:45:14.874131] [21:45:14.874371] [21:45:14.874367] [21:45:14.874395] [21:45:14.874363] [21:45:14.874424] [21:45:14.874419] [21:45:14.874447] [21:45:14.874358] [21:45:14.874481] [21:45:14.874476] [21:45:14.874511] [21:45:14.874472] [21:45:14.874540] [21:45:14.874535] [21:45:14.874565] [21:45:14.874127] [21:45:14.874608] [21:45:14.874603] [21:45:14.874632] [21:45:14.874599] [21:45:14.874660] [21:45:14.874655] [21:45:14.874684] [21:45:14.874594] [21:45:14.874721] [21:45:14.874717] [21:45:14.874749] [21:45:14.874707] [21:45:14.874779] [21:45:14.874774] [21:45:14.874803] [21:45:14.874590] [21:45:14.874841] [21:45:14.874837] [21:45:14.874865] [21:45:14.874832] [21:45:14.874893] [21:45:14.874889] [21:45:14.874917] [21:45:14.874828] [21:45:14.874954] [21:45:14.874949] [21:45:14.874978] [21:45:14.874945] [21:45:14.875006] [21:45:14.875001] [21:45:14.875029] blocks.0.attn.proj.weight torch.Size([768, 768])\n",
      "[21:45:14.875095] [21:45:14.875090] [21:45:14.875119] [21:45:14.875086] [21:45:14.875151] [21:45:14.875146] [21:45:14.875175] [21:45:14.875082] [21:45:14.875208] [21:45:14.875203] [21:45:14.875231] [21:45:14.875199] [21:45:14.875260] [21:45:14.875255] [21:45:14.875283] [21:45:14.875077] [21:45:14.875323] [21:45:14.875316] [21:45:14.875347] [21:45:14.875312] [21:45:14.875376] [21:45:14.875372] [21:45:14.875400] [21:45:14.875307] [21:45:14.875433] [21:45:14.875428] [21:45:14.875461] [21:45:14.875424] [21:45:14.875496] [21:45:14.875491] [21:45:14.875832] [21:45:14.875073] [21:45:14.875875] [21:45:14.875871] [21:45:14.875899] [21:45:14.875867] [21:45:14.875928] [21:45:14.875924] [21:45:14.875955] [21:45:14.875862] [21:45:14.875988] [21:45:14.875984] [21:45:14.876015] [21:45:14.875979] [21:45:14.876052] [21:45:14.876044] [21:45:14.876076] [21:45:14.875857] [21:45:14.876117] [21:45:14.876112] [21:45:14.876141] [21:45:14.876108] [21:45:14.876170] [21:45:14.876165] [21:45:14.876197] [21:45:14.876103] [21:45:14.876230] [21:45:14.876226] [21:45:14.876255] [21:45:14.876221] [21:45:14.876286] [21:45:14.876282] [21:45:14.876310] [21:45:14.875068] [21:45:14.876362] [21:45:14.876358] [21:45:14.876389] [21:45:14.876353] [21:45:14.876418] [21:45:14.876414] [21:45:14.876443] [21:45:14.876349] [21:45:14.876476] [21:45:14.876472] [21:45:14.876511] [21:45:14.876467] [21:45:14.876545] [21:45:14.876541] [21:45:14.876573] [21:45:14.876344] [21:45:14.876615] [21:45:14.876610] [21:45:14.876642] [21:45:14.876606] [21:45:14.876672] [21:45:14.876668] [21:45:14.876696] [21:45:14.876601] [21:45:14.876729] [21:45:14.876725] [21:45:14.876754] [21:45:14.876721] [21:45:14.876783] [21:45:14.876778] [21:45:14.876809] [21:45:14.876339] [21:45:14.876851] [21:45:14.876847] [21:45:14.876875] [21:45:14.876843] [21:45:14.876910] [21:45:14.876902] [21:45:14.876937] [21:45:14.876838] [21:45:14.876973] [21:45:14.876969] [21:45:14.876999] [21:45:14.876964] [21:45:14.877029] [21:45:14.877024] [21:45:14.877053] [21:45:14.876834] [21:45:14.877094] [21:45:14.877086] [21:45:14.877118] [21:45:14.877082] [21:45:14.877147] [21:45:14.877142] [21:45:14.877170] [21:45:14.877077] [21:45:14.877206] [21:45:14.877202] [21:45:14.877230] [21:45:14.877197] [21:45:14.877259] [21:45:14.877254] [21:45:14.877283] blocks.0.attn.proj.bias torch.Size([768])\n",
      "[21:45:14.877351] [21:45:14.877346] [21:45:14.877376] [21:45:14.877342] [21:45:14.877405] [21:45:14.877401] [21:45:14.877429] [21:45:14.877335] [21:45:14.877466] [21:45:14.877462] [21:45:14.877490] [21:45:14.877457] [21:45:14.877527] [21:45:14.877522] [21:45:14.877555] [21:45:14.877330] [21:45:14.877594] [21:45:14.877590] [21:45:14.877621] [21:45:14.877585] [21:45:14.877650] [21:45:14.877646] [21:45:14.877674] [21:45:14.877581] [21:45:14.877707] [21:45:14.877703] [21:45:14.877731] [21:45:14.877698] [21:45:14.877760] [21:45:14.877755] [21:45:14.877784] [21:45:14.877326] [21:45:14.877828] [21:45:14.877824] [21:45:14.877853] [21:45:14.877820] [21:45:14.877884] [21:45:14.877877] [21:45:14.877913] [21:45:14.877815] [21:45:14.877950] [21:45:14.877945] [21:45:14.877974] [21:45:14.877940] [21:45:14.878005] [21:45:14.878001] [21:45:14.878029] [21:45:14.877811] [21:45:14.878070] [21:45:14.878063] [21:45:14.878098] [21:45:14.878058] [21:45:14.878128] [21:45:14.878123] [21:45:14.878154] [21:45:14.878053] [21:45:14.878188] [21:45:14.878184] [21:45:14.878212] [21:45:14.878179] [21:45:14.878243] [21:45:14.878239] [21:45:14.878267] [21:45:14.877321] [21:45:14.878314] [21:45:14.878309] [21:45:14.878339] [21:45:14.878305] [21:45:14.878367] [21:45:14.878363] [21:45:14.878395] [21:45:14.878300] [21:45:14.878428] [21:45:14.878424] [21:45:14.878452] [21:45:14.878420] [21:45:14.878485] [21:45:14.878480] [21:45:14.878521] [21:45:14.878296] [21:45:14.878563] [21:45:14.878559] [21:45:14.878587] [21:45:14.878554] [21:45:14.878615] [21:45:14.878611] [21:45:14.878639] [21:45:14.878549] [21:45:14.878672] [21:45:14.878668] [21:45:14.878696] [21:45:14.878663] [21:45:14.878729] [21:45:14.878724] [21:45:14.878756] [21:45:14.878292] [21:45:14.878798] [21:45:14.878793] [21:45:14.878822] [21:45:14.878789] [21:45:14.878851] [21:45:14.878846] [21:45:14.878879] [21:45:14.878785] [21:45:14.878912] [21:45:14.878908] [21:45:14.878936] [21:45:14.878903] [21:45:14.878964] [21:45:14.878960] [21:45:14.878988] [21:45:14.878780] [21:45:14.879025] [21:45:14.879021] [21:45:14.879052] [21:45:14.879017] [21:45:14.879081] [21:45:14.879076] [21:45:14.879104] [21:45:14.879012] [21:45:14.879140] [21:45:14.879135] [21:45:14.879163] [21:45:14.879131] [21:45:14.879195] [21:45:14.879187] [21:45:14.879218] blocks.0.norm2.weight torch.Size([768])\n",
      "[21:45:14.879284] [21:45:14.879280] [21:45:14.879309] [21:45:14.879276] [21:45:14.879341] [21:45:14.879336] [21:45:14.879365] [21:45:14.879271] [21:45:14.879400] [21:45:14.879396] [21:45:14.879424] [21:45:14.879391] [21:45:14.879453] [21:45:14.879449] [21:45:14.879477] [21:45:14.879267] [21:45:14.879523] [21:45:14.879518] [21:45:14.879547] [21:45:14.879514] [21:45:14.879576] [21:45:14.879571] [21:45:14.879603] [21:45:14.879509] [21:45:14.879637] [21:45:14.879632] [21:45:14.879661] [21:45:14.879628] [21:45:14.879690] [21:45:14.879685] [21:45:14.879714] [21:45:14.879262] [21:45:14.879756] [21:45:14.879752] [21:45:14.879780] [21:45:14.879747] [21:45:14.879812] [21:45:14.879808] [21:45:14.879836] [21:45:14.879743] [21:45:14.879869] [21:45:14.879865] [21:45:14.879896] [21:45:14.879860] [21:45:14.879928] [21:45:14.879923] [21:45:14.879952] [21:45:14.879738] [21:45:14.879992] [21:45:14.879988] [21:45:14.880016] [21:45:14.879984] [21:45:14.880045] [21:45:14.880040] [21:45:14.880069] [21:45:14.879979] [21:45:14.880101] [21:45:14.880097] [21:45:14.880125] [21:45:14.880093] [21:45:14.880156] [21:45:14.880152] [21:45:14.880180] [21:45:14.879257] [21:45:14.880226] [21:45:14.880221] [21:45:14.880249] [21:45:14.880217] [21:45:14.880278] [21:45:14.880274] [21:45:14.880302] [21:45:14.880213] [21:45:14.880341] [21:45:14.880334] [21:45:14.880365] [21:45:14.880329] [21:45:14.880394] [21:45:14.880390] [21:45:14.880424] [21:45:14.880208] [21:45:14.880466] [21:45:14.880462] [21:45:14.880490] [21:45:14.880457] [21:45:14.880528] [21:45:14.880524] [21:45:14.880553] [21:45:14.880453] [21:45:14.880587] [21:45:14.880582] [21:45:14.880611] [21:45:14.880577] [21:45:14.880639] [21:45:14.880635] [21:45:14.880663] [21:45:14.880204] [21:45:14.880708] [21:45:14.880704] [21:45:14.880732] [21:45:14.880700] [21:45:14.880763] [21:45:14.880759] [21:45:14.880787] [21:45:14.880695] [21:45:14.880820] [21:45:14.880816] [21:45:14.880844] [21:45:14.880811] [21:45:14.880872] [21:45:14.880868] [21:45:14.880896] [21:45:14.880691] [21:45:14.880936] [21:45:14.880932] [21:45:14.880960] [21:45:14.880927] [21:45:14.880989] [21:45:14.880984] [21:45:14.881016] [21:45:14.880920] [21:45:14.881052] [21:45:14.881048] [21:45:14.881079] [21:45:14.881041] [21:45:14.881108] [21:45:14.881103] [21:45:14.881135] blocks.0.norm2.bias torch.Size([768])\n",
      "[21:45:14.881197] [21:45:14.881193] [21:45:14.881224] [21:45:14.881188] [21:45:14.881252] [21:45:14.881248] [21:45:14.881276] [21:45:14.881184] [21:45:14.881309] [21:45:14.881304] [21:45:14.881335] [21:45:14.881300] [21:45:14.881368] [21:45:14.881361] [21:45:14.881395] [21:45:14.881180] [21:45:14.881432] [21:45:14.881428] [21:45:14.881459] [21:45:14.881424] [21:45:14.881487] [21:45:14.881483] [21:45:14.881517] [21:45:14.881419] [21:45:14.881552] [21:45:14.881547] [21:45:14.881579] [21:45:14.881542] [21:45:14.881608] [21:45:14.881603] [21:45:14.881634] [21:45:14.881175] [21:45:14.881676] [21:45:14.881672] [21:45:14.881704] [21:45:14.881668] [21:45:14.881732] [21:45:14.881728] [21:45:14.881756] [21:45:14.881663] [21:45:14.881789] [21:45:14.881785] [21:45:14.881813] [21:45:14.881780] [21:45:14.881849] [21:45:14.881844] [21:45:14.881873] [21:45:14.881659] [21:45:14.881914] [21:45:14.881909] [21:45:14.881938] [21:45:14.881905] [21:45:14.881967] [21:45:14.881962] [21:45:14.881991] [21:45:14.881900] [21:45:14.882024] [21:45:14.882020] [21:45:14.882048] [21:45:14.882015] [21:45:14.882077] [21:45:14.882073] [21:45:14.882102] [21:45:14.881170] [21:45:14.882148] [21:45:14.882144] [21:45:14.882173] [21:45:14.882140] [21:45:14.882201] [21:45:14.882197] [21:45:14.882225] [21:45:14.882135] [21:45:14.882258] [21:45:14.882254] [21:45:14.882282] [21:45:14.882250] [21:45:14.882314] [21:45:14.882309] [21:45:14.882338] [21:45:14.882131] [21:45:14.882378] [21:45:14.882374] [21:45:14.882403] [21:45:14.882370] [21:45:14.882434] [21:45:14.882430] [21:45:14.882461] [21:45:14.882365] [21:45:14.882494] [21:45:14.882490] [21:45:14.882524] [21:45:14.882485] [21:45:14.882555] [21:45:14.882551] [21:45:14.882580] [21:45:14.882126] [21:45:14.882625] [21:45:14.882621] [21:45:14.882652] [21:45:14.882617] [21:45:14.882681] [21:45:14.882677] [21:45:14.882705] [21:45:14.882612] [21:45:14.882739] [21:45:14.882734] [21:45:14.882763] [21:45:14.882730] [21:45:14.882791] [21:45:14.882787] [21:45:14.882815] [21:45:14.882608] [21:45:14.882853] [21:45:14.882849] [21:45:14.882877] [21:45:14.882844] [21:45:14.882906] [21:45:14.882901] [21:45:14.882930] [21:45:14.882840] [21:45:14.882966] [21:45:14.882962] [21:45:14.882990] [21:45:14.882954] [21:45:14.883019] [21:45:14.883014] [21:45:14.883045] blocks.0.mlp.fc1.weight torch.Size([3072, 768])\n",
      "[21:45:14.883107] [21:45:14.883103] [21:45:14.883137] [21:45:14.883099] [21:45:14.883169] [21:45:14.883164] [21:45:14.883195] [21:45:14.883094] [21:45:14.883229] [21:45:14.883225] [21:45:14.883253] [21:45:14.883220] [21:45:14.883282] [21:45:14.883277] [21:45:14.883306] [21:45:14.883090] [21:45:14.883361] [21:45:14.883356] [21:45:14.883386] [21:45:14.883336] [21:45:14.883415] [21:45:14.883410] [21:45:14.883439] [21:45:14.883331] [21:45:14.883472] [21:45:14.883468] [21:45:14.883497] [21:45:14.883463] [21:45:14.883533] [21:45:14.883528] [21:45:14.883557] [21:45:14.883086] [21:45:14.883599] [21:45:14.883594] [21:45:14.883623] [21:45:14.883590] [21:45:14.883656] [21:45:14.883651] [21:45:14.883680] [21:45:14.883586] [21:45:14.883716] [21:45:14.883711] [21:45:14.883742] [21:45:14.883706] [21:45:14.883771] [21:45:14.883766] [21:45:14.883800] [21:45:14.883581] [21:45:14.883838] [21:45:14.883834] [21:45:14.883863] [21:45:14.883829] [21:45:14.883891] [21:45:14.883887] [21:45:14.883916] [21:45:14.883825] [21:45:14.883952] [21:45:14.883947] [21:45:14.883976] [21:45:14.883940] [21:45:14.884005] [21:45:14.884000] [21:45:14.884030] [21:45:14.883081] [21:45:14.884077] [21:45:14.884072] [21:45:14.884107] [21:45:14.884068] [21:45:14.884136] [21:45:14.884131] [21:45:14.884160] [21:45:14.884064] [21:45:14.884199] [21:45:14.884192] [21:45:14.884228] [21:45:14.884187] [21:45:14.884257] [21:45:14.884252] [21:45:14.884284] [21:45:14.884059] [21:45:14.884322] [21:45:14.884318] [21:45:14.884346] [21:45:14.884313] [21:45:14.884374] [21:45:14.884370] [21:45:14.884398] [21:45:14.884309] [21:45:14.884431] [21:45:14.884427] [21:45:14.884458] [21:45:14.884422] [21:45:14.884487] [21:45:14.884482] [21:45:14.884534] [21:45:14.884055] [21:45:14.884577] [21:45:14.884572] [21:45:14.884601] [21:45:14.884568] [21:45:14.884630] [21:45:14.884625] [21:45:14.884653] [21:45:14.884564] [21:45:14.884693] [21:45:14.884688] [21:45:14.884717] [21:45:14.884684] [21:45:14.884746] [21:45:14.884741] [21:45:14.884774] [21:45:14.884559] [21:45:14.884815] [21:45:14.884811] [21:45:14.884839] [21:45:14.884804] [21:45:14.884868] [21:45:14.884863] [21:45:14.884897] [21:45:14.884799] [21:45:14.884931] [21:45:14.884926] [21:45:14.884954] [21:45:14.884921] [21:45:14.884986] [21:45:14.884978] [21:45:14.885010] blocks.0.mlp.fc1.bias torch.Size([3072])\n",
      "[21:45:14.885075] [21:45:14.885071] [21:45:14.885107] [21:45:14.885066] [21:45:14.885135] [21:45:14.885131] [21:45:14.885159] [21:45:14.885059] [21:45:14.885194] [21:45:14.885190] [21:45:14.885218] [21:45:14.885185] [21:45:14.885247] [21:45:14.885242] [21:45:14.885271] [21:45:14.885055] [21:45:14.885311] [21:45:14.885307] [21:45:14.885338] [21:45:14.885302] [21:45:14.885366] [21:45:14.885361] [21:45:14.885390] [21:45:14.885298] [21:45:14.885426] [21:45:14.885421] [21:45:14.885450] [21:45:14.885417] [21:45:14.885478] [21:45:14.885474] [21:45:14.885510] [21:45:14.885051] [21:45:14.885557] [21:45:14.885549] [21:45:14.885581] [21:45:14.885545] [21:45:14.885610] [21:45:14.885605] [21:45:14.885637] [21:45:14.885540] [21:45:14.885671] [21:45:14.885666] [21:45:14.885695] [21:45:14.885662] [21:45:14.885723] [21:45:14.885719] [21:45:14.885747] [21:45:14.885535] [21:45:14.885788] [21:45:14.885784] [21:45:14.885815] [21:45:14.885779] [21:45:14.885845] [21:45:14.885840] [21:45:14.885872] [21:45:14.885775] [21:45:14.885906] [21:45:14.885902] [21:45:14.885933] [21:45:14.885897] [21:45:14.885962] [21:45:14.885958] [21:45:14.885986] [21:45:14.885046] [21:45:14.886033] [21:45:14.886028] [21:45:14.886060] [21:45:14.886024] [21:45:14.886089] [21:45:14.886085] [21:45:14.886113] [21:45:14.886020] [21:45:14.886149] [21:45:14.886145] [21:45:14.886176] [21:45:14.886140] [21:45:14.886204] [21:45:14.886200] [21:45:14.886231] [21:45:14.886015] [21:45:14.886269] [21:45:14.886265] [21:45:14.886293] [21:45:14.886260] [21:45:14.886322] [21:45:14.886317] [21:45:14.886346] [21:45:14.886255] [21:45:14.886385] [21:45:14.886378] [21:45:14.886409] [21:45:14.886373] [21:45:14.886437] [21:45:14.886433] [21:45:14.886461] [21:45:14.886011] [21:45:14.886509] [21:45:14.886498] [21:45:14.886537] [21:45:14.886494] [21:45:14.886566] [21:45:14.886561] [21:45:14.886589] [21:45:14.886490] [21:45:14.886623] [21:45:14.886618] [21:45:14.886647] [21:45:14.886614] [21:45:14.886676] [21:45:14.886671] [21:45:14.886700] [21:45:14.886485] [21:45:14.886740] [21:45:14.886736] [21:45:14.886769] [21:45:14.886729] [21:45:14.886801] [21:45:14.886796] [21:45:14.886825] [21:45:14.886724] [21:45:14.886861] [21:45:14.886856] [21:45:14.886887] [21:45:14.886852] [21:45:14.886916] [21:45:14.886911] [21:45:14.886942] blocks.0.mlp.fc2.weight torch.Size([768, 3072])\n",
      "[21:45:14.887007] [21:45:14.887003] [21:45:14.887035] [21:45:14.886999] [21:45:14.887067] [21:45:14.887062] [21:45:14.887091] [21:45:14.886994] [21:45:14.887124] [21:45:14.887120] [21:45:14.887153] [21:45:14.887115] [21:45:14.887182] [21:45:14.887177] [21:45:14.887206] [21:45:14.886990] [21:45:14.887244] [21:45:14.887239] [21:45:14.887268] [21:45:14.887235] [21:45:14.887296] [21:45:14.887292] [21:45:14.887320] [21:45:14.887230] [21:45:14.887357] [21:45:14.887352] [21:45:14.887381] [21:45:14.887348] [21:45:14.887415] [21:45:14.887408] [21:45:14.887444] [21:45:14.886985] [21:45:14.887487] [21:45:14.887482] [21:45:14.887516] [21:45:14.887478] [21:45:14.887548] [21:45:14.887543] [21:45:14.887572] [21:45:14.887474] [21:45:14.887606] [21:45:14.887601] [21:45:14.887630] [21:45:14.887597] [21:45:14.887662] [21:45:14.887657] [21:45:14.887686] [21:45:14.887469] [21:45:14.887727] [21:45:14.887722] [21:45:14.887754] [21:45:14.887715] [21:45:14.887786] [21:45:14.887781] [21:45:14.887810] [21:45:14.887711] [21:45:14.887843] [21:45:14.887839] [21:45:14.887867] [21:45:14.887834] [21:45:14.887896] [21:45:14.887891] [21:45:14.887924] [21:45:14.886978] [21:45:14.887974] [21:45:14.887970] [21:45:14.887998] [21:45:14.887965] [21:45:14.888027] [21:45:14.888022] [21:45:14.888050] [21:45:14.887957] [21:45:14.888086] [21:45:14.888082] [21:45:14.888110] [21:45:14.888075] [21:45:14.888139] [21:45:14.888134] [21:45:14.888163] [21:45:14.887953] [21:45:14.888203] [21:45:14.888196] [21:45:14.888232] [21:45:14.888191] [21:45:14.888264] [21:45:14.888260] [21:45:14.888288] [21:45:14.888187] [21:45:14.888324] [21:45:14.888319] [21:45:14.888348] [21:45:14.888315] [21:45:14.888379] [21:45:14.888375] [21:45:14.888403] [21:45:14.887948] [21:45:14.888449] [21:45:14.888444] [21:45:14.888472] [21:45:14.888440] [21:45:14.888508] [21:45:14.888496] [21:45:14.888534] [21:45:14.888435] [21:45:14.888568] [21:45:14.888563] [21:45:14.888591] [21:45:14.888559] [21:45:14.888624] [21:45:14.888619] [21:45:14.888647] [21:45:14.888431] [21:45:14.888691] [21:45:14.888686] [21:45:14.888715] [21:45:14.888679] [21:45:14.888743] [21:45:14.888739] [21:45:14.888769] [21:45:14.888675] [21:45:14.888808] [21:45:14.888801] [21:45:14.888832] [21:45:14.888797] [21:45:14.888860] [21:45:14.888856] [21:45:14.888886] blocks.0.mlp.fc2.bias torch.Size([768])\n",
      "[21:45:14.888948] [21:45:14.888944] [21:45:14.888972] [21:45:14.888939] [21:45:14.889000] [21:45:14.888996] [21:45:14.889024] [21:45:14.888935] [21:45:14.889057] [21:45:14.889052] [21:45:14.889080] [21:45:14.889047] [21:45:14.889108] [21:45:14.889104] [21:45:14.889132] [21:45:14.888931] [21:45:14.889172] [21:45:14.889167] [21:45:14.889196] [21:45:14.889160] [21:45:14.889224] [21:45:14.889219] [21:45:14.889247] [21:45:14.889156] [21:45:14.889281] [21:45:14.889276] [21:45:14.889310] [21:45:14.889271] [21:45:14.889344] [21:45:14.889339] [21:45:14.889368] [21:45:14.888926] [21:45:14.889416] [21:45:14.889411] [21:45:14.889440] [21:45:14.889404] [21:45:14.889469] [21:45:14.889464] [21:45:14.889496] [21:45:14.889399] [21:45:14.889537] [21:45:14.889533] [21:45:14.889563] [21:45:14.889528] [21:45:14.889595] [21:45:14.889590] [21:45:14.889619] [21:45:14.889395] [21:45:14.889659] [21:45:14.889655] [21:45:14.889684] [21:45:14.889648] [21:45:14.889718] [21:45:14.889711] [21:45:14.889742] [21:45:14.889643] [21:45:14.889778] [21:45:14.889771] [21:45:14.889802] [21:45:14.889767] [21:45:14.889831] [21:45:14.889826] [21:45:14.889855] [21:45:14.888922] [21:45:14.889902] [21:45:14.889898] [21:45:14.889926] [21:45:14.889893] [21:45:14.889958] [21:45:14.889954] [21:45:14.889982] [21:45:14.889889] [21:45:14.890016] [21:45:14.890012] [21:45:14.890040] [21:45:14.890007] [21:45:14.890074] [21:45:14.890067] [21:45:14.890104] [21:45:14.889884] [21:45:14.890141] [21:45:14.890137] [21:45:14.890172] [21:45:14.890133] [21:45:14.890201] [21:45:14.890197] [21:45:14.890225] [21:45:14.890128] [21:45:14.890261] [21:45:14.890257] [21:45:14.890285] [21:45:14.890252] [21:45:14.890314] [21:45:14.890309] [21:45:14.890341] [21:45:14.889880] [21:45:14.890386] [21:45:14.890382] [21:45:14.890414] [21:45:14.890377] [21:45:14.890446] [21:45:14.890441] [21:45:14.890472] [21:45:14.890373] [21:45:14.890515] [21:45:14.890510] [21:45:14.890539] [21:45:14.890497] [21:45:14.890568] [21:45:14.890563] [21:45:14.890592] [21:45:14.890365] [21:45:14.890630] [21:45:14.890626] [21:45:14.890654] [21:45:14.890621] [21:45:14.890683] [21:45:14.890679] [21:45:14.890707] [21:45:14.890617] [21:45:14.890743] [21:45:14.890739] [21:45:14.890768] [21:45:14.890734] [21:45:14.890796] [21:45:14.890792] [21:45:14.890820] blocks.1.norm1.weight torch.Size([768])\n",
      "[21:45:14.890882] [21:45:14.890878] [21:45:14.890912] [21:45:14.890873] [21:45:14.890944] [21:45:14.890939] [21:45:14.890968] [21:45:14.890869] [21:45:14.891002] [21:45:14.890998] [21:45:14.891029] [21:45:14.890993] [21:45:14.891059] [21:45:14.891054] [21:45:14.891083] [21:45:14.890865] [21:45:14.891126] [21:45:14.891122] [21:45:14.891151] [21:45:14.891118] [21:45:14.891183] [21:45:14.891178] [21:45:14.891207] [21:45:14.891113] [21:45:14.891241] [21:45:14.891236] [21:45:14.891265] [21:45:14.891231] [21:45:14.891297] [21:45:14.891292] [21:45:14.891323] [21:45:14.890860] [21:45:14.891371] [21:45:14.891367] [21:45:14.891395] [21:45:14.891362] [21:45:14.891427] [21:45:14.891422] [21:45:14.891452] [21:45:14.891358] [21:45:14.891488] [21:45:14.891483] [21:45:14.891519] [21:45:14.891479] [21:45:14.891548] [21:45:14.891543] [21:45:14.891571] [21:45:14.891353] [21:45:14.891610] [21:45:14.891606] [21:45:14.891637] [21:45:14.891601] [21:45:14.891666] [21:45:14.891661] [21:45:14.891690] [21:45:14.891597] [21:45:14.891732] [21:45:14.891725] [21:45:14.891761] [21:45:14.891720] [21:45:14.891790] [21:45:14.891785] [21:45:14.891816] [21:45:14.890855] [21:45:14.891863] [21:45:14.891858] [21:45:14.891889] [21:45:14.891854] [21:45:14.891918] [21:45:14.891914] [21:45:14.891942] [21:45:14.891850] [21:45:14.891979] [21:45:14.891974] [21:45:14.892003] [21:45:14.891969] [21:45:14.892036] [21:45:14.892027] [21:45:14.892065] [21:45:14.891845] [21:45:14.892106] [21:45:14.892101] [21:45:14.892132] [21:45:14.892097] [21:45:14.892164] [21:45:14.892159] [21:45:14.892187] [21:45:14.892090] [21:45:14.892228] [21:45:14.892223] [21:45:14.892252] [21:45:14.892216] [21:45:14.892281] [21:45:14.892276] [21:45:14.892305] [21:45:14.891841] [21:45:14.892346] [21:45:14.892342] [21:45:14.892370] [21:45:14.892338] [21:45:14.892399] [21:45:14.892394] [21:45:14.892425] [21:45:14.892333] [21:45:14.892459] [21:45:14.892454] [21:45:14.892485] [21:45:14.892449] [21:45:14.892531] [21:45:14.892522] [21:45:14.892561] [21:45:14.892329] [21:45:14.892601] [21:45:14.892595] [21:45:14.892625] [21:45:14.892590] [21:45:14.892654] [21:45:14.892649] [21:45:14.892678] [21:45:14.892586] [21:45:14.892714] [21:45:14.892710] [21:45:14.892738] [21:45:14.892705] [21:45:14.892769] [21:45:14.892764] [21:45:14.892793] blocks.1.norm1.bias torch.Size([768])\n",
      "[21:45:14.892858] [21:45:14.892853] [21:45:14.892884] [21:45:14.892846] [21:45:14.892916] [21:45:14.892911] [21:45:14.892940] [21:45:14.892842] [21:45:14.892976] [21:45:14.892971] [21:45:14.892999] [21:45:14.892966] [21:45:14.893030] [21:45:14.893026] [21:45:14.893054] [21:45:14.892838] [21:45:14.893094] [21:45:14.893090] [21:45:14.893118] [21:45:14.893085] [21:45:14.893146] [21:45:14.893142] [21:45:14.893178] [21:45:14.893081] [21:45:14.893214] [21:45:14.893209] [21:45:14.893245] [21:45:14.893205] [21:45:14.893274] [21:45:14.893269] [21:45:14.893297] [21:45:14.892833] [21:45:14.893343] [21:45:14.893338] [21:45:14.893367] [21:45:14.893334] [21:45:14.893398] [21:45:14.893394] [21:45:14.893423] [21:45:14.893329] [21:45:14.893456] [21:45:14.893452] [21:45:14.893480] [21:45:14.893447] [21:45:14.893519] [21:45:14.893514] [21:45:14.893546] [21:45:14.893322] [21:45:14.893584] [21:45:14.893579] [21:45:14.893608] [21:45:14.893575] [21:45:14.893639] [21:45:14.893634] [21:45:14.893663] [21:45:14.893570] [21:45:14.893699] [21:45:14.893694] [21:45:14.893723] [21:45:14.893689] [21:45:14.893751] [21:45:14.893747] [21:45:14.893775] [21:45:14.892828] [21:45:14.893822] [21:45:14.893817] [21:45:14.893846] [21:45:14.893813] [21:45:14.893877] [21:45:14.893873] [21:45:14.893901] [21:45:14.893809] [21:45:14.893934] [21:45:14.893930] [21:45:14.893958] [21:45:14.893925] [21:45:14.893990] [21:45:14.893986] [21:45:14.894017] [21:45:14.893804] [21:45:14.894054] [21:45:14.894050] [21:45:14.894080] [21:45:14.894045] [21:45:14.894111] [21:45:14.894106] [21:45:14.894141] [21:45:14.894041] [21:45:14.894182] [21:45:14.894177] [21:45:14.894208] [21:45:14.894170] [21:45:14.894237] [21:45:14.894233] [21:45:14.894261] [21:45:14.893800] [21:45:14.894302] [21:45:14.894298] [21:45:14.894326] [21:45:14.894294] [21:45:14.894354] [21:45:14.894350] [21:45:14.894380] [21:45:14.894290] [21:45:14.894414] [21:45:14.894409] [21:45:14.894437] [21:45:14.894404] [21:45:14.894465] [21:45:14.894461] [21:45:14.894489] [21:45:14.894285] [21:45:14.894534] [21:45:14.894530] [21:45:14.894558] [21:45:14.894525] [21:45:14.894587] [21:45:14.894582] [21:45:14.894612] [21:45:14.894520] [21:45:14.894646] [21:45:14.894642] [21:45:14.894670] [21:45:14.894637] [21:45:14.894698] [21:45:14.894694] [21:45:14.894722] blocks.1.attn.qkv.weight torch.Size([2304, 768])\n",
      "[21:45:14.894962] [21:45:14.894958] [21:45:14.894987] [21:45:14.894954] [21:45:14.895019] [21:45:14.895014] [21:45:14.895048] [21:45:14.894949] [21:45:14.895082] [21:45:14.895078] [21:45:14.895108] [21:45:14.895073] [21:45:14.895137] [21:45:14.895133] [21:45:14.895161] [21:45:14.894945] [21:45:14.895199] [21:45:14.895195] [21:45:14.895223] [21:45:14.895190] [21:45:14.895252] [21:45:14.895247] [21:45:14.895276] [21:45:14.895186] [21:45:14.895310] [21:45:14.895305] [21:45:14.895336] [21:45:14.895301] [21:45:14.895365] [21:45:14.895360] [21:45:14.895391] [21:45:14.894941] [21:45:14.895437] [21:45:14.895432] [21:45:14.895461] [21:45:14.895428] [21:45:14.895490] [21:45:14.895485] [21:45:14.895530] [21:45:14.895420] [21:45:14.895569] [21:45:14.895564] [21:45:14.895592] [21:45:14.895559] [21:45:14.895629] [21:45:14.895625] [21:45:14.895653] [21:45:14.895416] [21:45:14.896518] [21:45:14.895686] [21:45:14.896552] [21:45:14.895682] [21:45:14.896584] [21:45:14.896579] [21:45:14.896608] [21:45:14.895677] [21:45:14.896644] [21:45:14.896640] [21:45:14.896668] [21:45:14.896635] [21:45:14.896697] [21:45:14.896692] [21:45:14.896721] [21:45:14.894936] [21:45:14.896771] [21:45:14.896766] [21:45:14.896795] [21:45:14.896762] [21:45:14.896826] [21:45:14.896821] [21:45:14.896850] [21:45:14.896757] [21:45:14.896886] [21:45:14.896881] [21:45:14.896910] [21:45:14.896877] [21:45:14.896941] [21:45:14.896937] [21:45:14.896965] [21:45:14.896750] [21:45:14.897002] [21:45:14.896998] [21:45:14.897026] [21:45:14.896994] [21:45:14.897055] [21:45:14.897051] [21:45:14.897079] [21:45:14.896989] [21:45:14.897112] [21:45:14.897108] [21:45:14.897137] [21:45:14.897103] [21:45:14.897165] [21:45:14.897161] [21:45:14.897189] [21:45:14.896745] [21:45:14.897231] [21:45:14.897227] [21:45:14.897794] [21:45:14.897222] [21:45:14.897824] [21:45:14.897819] [21:45:14.897847] [21:45:14.897218] [21:45:14.897884] [21:45:14.897879] [21:45:14.897907] [21:45:14.897874] [21:45:14.897936] [21:45:14.897931] [21:45:14.897960] [21:45:14.897213] [21:45:14.897998] [21:45:14.897994] [21:45:14.898022] [21:45:14.897989] [21:45:14.898050] [21:45:14.898046] [21:45:14.898077] [21:45:14.897985] [21:45:14.898110] [21:45:14.898106] [21:45:14.898134] [21:45:14.898101] [21:45:14.898163] [21:45:14.898158] [21:45:14.898186] blocks.1.attn.qkv.bias torch.Size([2304])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from rdm.pretrained_enc import models_pretrained_enc\n",
    "\n",
    "proj_dim = 256\n",
    "model = models_pretrained_enc.mocov3_vit_base(proj_dim=proj_dim)\n",
    "\n",
    "model_state = model.state_dict()\n",
    "print(\"model keys:\", len(model_state))\n",
    "print(\"first 20 model keys:\")\n",
    "for k in list(model_state.keys())[:20]:\n",
    "    print(k, model_state[k].shape)\n",
    "\n",
    "# Optional: compare with checkpoint\n",
    "ckpt_path = \"pretrained_enc_ckpts/mocov3/rdm-mocov3vitb.pth\"\n",
    "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "state = ckpt[\"model\"] if isinstance(ckpt, dict) and \"model\" in ckpt else (\n",
    "    ckpt[\"state_dict\"] if isinstance(ckpt, dict) and \"state_dict\" in ckpt else ckpt\n",
    ")\n",
    "\n",
    "missing = [k for k in model_state if k not in state]\n",
    "extra = [k for k in state if k not in model_state]\n",
    "mismatch = [(k, state[k].shape, model_state[k].shape) for k in state if k in model_state and state[k].shape != model_state[k].shape]\n",
    "\n",
    "print(\"missing in ckpt:\", len(missing))\n",
    "print(\"extra in ckpt:\", len(extra))\n",
    "print(\"shape mismatches:\", len(mismatch))\n",
    "print(\"sample mismatches:\", mismatch[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "477f06da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt keys: ['model']\n",
      "\n",
      "model key count: 275\n",
      "prefixes: [('model_ema', 132), ('model', 130), ('betas', 1), ('alphas_cumprod', 1), ('alphas_cumprod_prev', 1), ('sqrt_alphas_cumprod', 1), ('sqrt_one_minus_alphas_cumprod', 1), ('log_one_minus_alphas_cumprod', 1), ('sqrt_recip_alphas_cumprod', 1), ('sqrt_recipm1_alphas_cumprod', 1), ('posterior_variance', 1), ('posterior_log_variance_clipped', 1), ('posterior_mean_coef1', 1), ('posterior_mean_coef2', 1), ('cond_stage_model', 1)]\n",
      "first 20 keys:\n",
      "betas torch.Size([1000])\n",
      "alphas_cumprod torch.Size([1000])\n",
      "alphas_cumprod_prev torch.Size([1000])\n",
      "sqrt_alphas_cumprod torch.Size([1000])\n",
      "sqrt_one_minus_alphas_cumprod torch.Size([1000])\n",
      "log_one_minus_alphas_cumprod torch.Size([1000])\n",
      "sqrt_recip_alphas_cumprod torch.Size([1000])\n",
      "sqrt_recipm1_alphas_cumprod torch.Size([1000])\n",
      "posterior_variance torch.Size([1000])\n",
      "posterior_log_variance_clipped torch.Size([1000])\n",
      "posterior_mean_coef1 torch.Size([1000])\n",
      "posterior_mean_coef2 torch.Size([1000])\n",
      "model.diffusion_model.time_embed.0.weight torch.Size([256, 1536])\n",
      "model.diffusion_model.time_embed.0.bias torch.Size([256])\n",
      "model.diffusion_model.time_embed.2.weight torch.Size([256, 256])\n",
      "model.diffusion_model.time_embed.2.bias torch.Size([256])\n",
      "model.diffusion_model.input_proj.weight torch.Size([1536, 256])\n",
      "model.diffusion_model.input_proj.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.0.in_layers.0.weight torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.0.in_layers.0.bias torch.Size([1536])\n",
      "\n",
      "encoder-like keys: 240\n",
      "model.diffusion_model.res_blocks.0.in_layers.0.weight torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.0.in_layers.0.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.0.in_layers.2.weight torch.Size([1536, 1536])\n",
      "model.diffusion_model.res_blocks.0.in_layers.2.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.0.emb_layers.1.weight torch.Size([1536, 256])\n",
      "model.diffusion_model.res_blocks.0.emb_layers.1.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.0.out_layers.0.weight torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.0.out_layers.0.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.0.out_layers.3.weight torch.Size([1536, 1536])\n",
      "model.diffusion_model.res_blocks.0.out_layers.3.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.1.in_layers.0.weight torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.1.in_layers.0.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.1.in_layers.2.weight torch.Size([1536, 1536])\n",
      "model.diffusion_model.res_blocks.1.in_layers.2.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.1.emb_layers.1.weight torch.Size([1536, 256])\n",
      "model.diffusion_model.res_blocks.1.emb_layers.1.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.1.out_layers.0.weight torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.1.out_layers.0.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.1.out_layers.3.weight torch.Size([1536, 1536])\n",
      "model.diffusion_model.res_blocks.1.out_layers.3.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.2.in_layers.0.weight torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.2.in_layers.0.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.2.in_layers.2.weight torch.Size([1536, 1536])\n",
      "model.diffusion_model.res_blocks.2.in_layers.2.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.2.emb_layers.1.weight torch.Size([1536, 256])\n",
      "model.diffusion_model.res_blocks.2.emb_layers.1.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.2.out_layers.0.weight torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.2.out_layers.0.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.2.out_layers.3.weight torch.Size([1536, 1536])\n",
      "model.diffusion_model.res_blocks.2.out_layers.3.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.3.in_layers.0.weight torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.3.in_layers.0.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.3.in_layers.2.weight torch.Size([1536, 1536])\n",
      "model.diffusion_model.res_blocks.3.in_layers.2.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.3.emb_layers.1.weight torch.Size([1536, 256])\n",
      "model.diffusion_model.res_blocks.3.emb_layers.1.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.3.out_layers.0.weight torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.3.out_layers.0.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.3.out_layers.3.weight torch.Size([1536, 1536])\n",
      "model.diffusion_model.res_blocks.3.out_layers.3.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.4.in_layers.0.weight torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.4.in_layers.0.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.4.in_layers.2.weight torch.Size([1536, 1536])\n",
      "model.diffusion_model.res_blocks.4.in_layers.2.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.4.emb_layers.1.weight torch.Size([1536, 256])\n",
      "model.diffusion_model.res_blocks.4.emb_layers.1.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.4.out_layers.0.weight torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.4.out_layers.0.bias torch.Size([1536])\n",
      "model.diffusion_model.res_blocks.4.out_layers.3.weight torch.Size([1536, 1536])\n",
      "model.diffusion_model.res_blocks.4.out_layers.3.bias torch.Size([1536])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "ckpt_path = \"pretrained_enc_ckpts/mocov3/rdm-mocov3vitb.pth\"  # update if needed\n",
    "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "print(\"ckpt keys:\", list(ckpt.keys()) if isinstance(ckpt, dict) else type(ckpt))\n",
    "\n",
    "def summarize(sd, name):\n",
    "    print(f\"\\n{name} key count:\", len(sd))\n",
    "    prefixes = Counter(k.split(\".\")[0] for k in sd.keys())\n",
    "    print(\"prefixes:\", prefixes.most_common(20))\n",
    "    print(\"first 20 keys:\")\n",
    "    for k in list(sd.keys())[:20]:\n",
    "        v = sd[k]\n",
    "        print(k, getattr(v, \"shape\", None))\n",
    "\n",
    "def find_keys(sd, needles):\n",
    "    hits = [k for k in sd if any(n in k for n in needles)]\n",
    "    print(\"\\nencoder-like keys:\", len(hits))\n",
    "    for k in hits[:50]:\n",
    "        print(k, sd[k].shape)\n",
    "\n",
    "if isinstance(ckpt, dict) and \"model\" in ckpt:\n",
    "    summarize(ckpt[\"model\"], \"model\")\n",
    "    find_keys(ckpt[\"model\"], [\"base_encoder\", \"backbone\", \"encoder\", \"vit\", \"patch_embed\", \"blocks\", \"head\"])\n",
    "if isinstance(ckpt, dict) and \"model_ema\" in ckpt:\n",
    "    summarize(ckpt[\"model_ema\"], \"model_ema\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3fe211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
