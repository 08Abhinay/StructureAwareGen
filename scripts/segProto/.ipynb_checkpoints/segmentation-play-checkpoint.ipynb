{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "796cf4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abelde/.local/lib/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AMG-prod:   0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 720.19 MiB is free. Process 1728920 has 396.00 MiB memory in use. Process 775485 has 224.00 MiB memory in use. Process 3628798 has 21.38 GiB memory in use. Including non-PyTorch memory, this process has 798.00 MiB memory in use. Of the allocated memory 451.72 MiB is allocated by PyTorch, and 70.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 232\u001b[0m\n\u001b[1;32m    229\u001b[0m H, W, _ \u001b[38;5;241m=\u001b[39m img_rgb\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# 1) Generate proposals\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m amg \u001b[38;5;241m=\u001b[39m \u001b[43mmask_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# 2) Compute image embedding once (optional but powerful)\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m SAVE_MASK_EMB:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/gilbreth/abelde/Thesis/StructureAwareGen/SegmentationAwareGen/lib/python3.9/site-packages/segment_anything/automatic_mask_generator.py:163\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator.generate\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03mGenerates masks for the given image.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m         the mask, given in XYWH format.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Generate masks\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m mask_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Filter small disconnected regions and holes in masks\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_mask_region_area \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/scratch/gilbreth/abelde/Thesis/StructureAwareGen/SegmentationAwareGen/lib/python3.9/site-packages/segment_anything/automatic_mask_generator.py:206\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._generate_masks\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    204\u001b[0m data \u001b[38;5;241m=\u001b[39m MaskData()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m crop_box, layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(crop_boxes, layer_idxs):\n\u001b[0;32m--> 206\u001b[0m     crop_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     data\u001b[38;5;241m.\u001b[39mcat(crop_data)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# Remove duplicate masks between crops\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/gilbreth/abelde/Thesis/StructureAwareGen/SegmentationAwareGen/lib/python3.9/site-packages/segment_anything/automatic_mask_generator.py:236\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._process_crop\u001b[0;34m(self, image, crop_box, crop_layer_idx, orig_size)\u001b[0m\n\u001b[1;32m    234\u001b[0m cropped_im \u001b[38;5;241m=\u001b[39m image[y0:y1, x0:x1, :]\n\u001b[1;32m    235\u001b[0m cropped_im_size \u001b[38;5;241m=\u001b[39m cropped_im\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcropped_im\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Get points for this crop\u001b[39;00m\n\u001b[1;32m    239\u001b[0m points_scale \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(cropped_im_size)[\u001b[38;5;28;01mNone\u001b[39;00m, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/scratch/gilbreth/abelde/Thesis/StructureAwareGen/SegmentationAwareGen/lib/python3.9/site-packages/segment_anything/predictor.py:60\u001b[0m, in \u001b[0;36mSamPredictor.set_image\u001b[0;34m(self, image, image_format)\u001b[0m\n\u001b[1;32m     57\u001b[0m input_image_torch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(input_image, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     58\u001b[0m input_image_torch \u001b[38;5;241m=\u001b[39m input_image_torch\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()[\u001b[38;5;28;01mNone\u001b[39;00m, :, :, :]\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_torch_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/gilbreth/abelde/Thesis/StructureAwareGen/SegmentationAwareGen/lib/python3.9/site-packages/segment_anything/predictor.py:89\u001b[0m, in \u001b[0;36mSamPredictor.set_torch_image\u001b[0;34m(self, transformed_image, original_image_size)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(transformed_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n\u001b[1;32m     88\u001b[0m input_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpreprocess(transformed_image)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_image_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/scratch/gilbreth/abelde/Thesis/StructureAwareGen/SegmentationAwareGen/lib/python3.9/site-packages/segment_anything/modeling/image_encoder.py:112\u001b[0m, in \u001b[0;36mImageEncoderViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 112\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck(x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/scratch/gilbreth/abelde/Thesis/StructureAwareGen/SegmentationAwareGen/lib/python3.9/site-packages/segment_anything/modeling/image_encoder.py:174\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m     H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    172\u001b[0m     x, pad_hw \u001b[38;5;241m=\u001b[39m window_partition(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[0;32m--> 174\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/scratch/gilbreth/abelde/Thesis/StructureAwareGen/SegmentationAwareGen/lib/python3.9/site-packages/segment_anything/modeling/image_encoder.py:231\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# q, k, v with shape (B * nHead, H * W, C)\u001b[39;00m\n\u001b[1;32m    229\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m3\u001b[39m, B \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H \u001b[38;5;241m*\u001b[39m W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 231\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_rel_pos:\n\u001b[1;32m    234\u001b[0m     attn \u001b[38;5;241m=\u001b[39m add_decomposed_rel_pos(attn, q, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_pos_h, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_pos_w, (H, W), (H, W))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 720.19 MiB is free. Process 1728920 has 396.00 MiB memory in use. Process 775485 has 224.00 MiB memory in use. Process 3628798 has 21.38 GiB memory in use. Including non-PyTorch memory, this process has 798.00 MiB memory in use. Of the allocated memory 451.72 MiB is allocated by PyTorch, and 70.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os, glob, json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "from segment_anything import sam_model_registry,  SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "# =========================\n",
    "# CONFIG (edit these)\n",
    "# =========================\n",
    "IN_DIR = \"/scratch/gilbreth/abelde/Thesis/StructureAwareGen/dataset/val2017\"\n",
    "OUT_ROOT = \"/scratch/gilbreth/abelde/Thesis/StructureAwareGen/scripts/segProto/out_amg_prod-2-trial\"\n",
    "CHECKPOINT = \"/scratch/gilbreth/abelde/Thesis/StructureAwareGen/scripts/segProto/checkpoints/sam_vit_b_01ec64.pth\"\n",
    "MODEL_TYPE = \"vit_b\"       # vit_b / vit_l / vit_h\n",
    "MAX_IMAGES = 5             # -1 for all\n",
    "\n",
    "# AMG \"segment everything\" knobs (balanced)\n",
    "POINTS_PER_SIDE = 64       # 128 is heavier; 256 is often overkill\n",
    "CROP_N_LAYERS = 1          # 0 = no crops, 1-2 helps small objects a lot\n",
    "CROP_OVERLAP_RATIO = 0.35\n",
    "CROP_N_POINTS_DOWNSCALE = 2\n",
    "\n",
    "PRED_IOU_THRESH = 0.80\n",
    "STABILITY_SCORE_THRESH = 0.85\n",
    "BOX_NMS_THRESH = 0.70\n",
    "\n",
    "# OpenCV-free post filter\n",
    "MIN_MASK_REGION_AREA = 300     # raise (500~2000) to remove tiny junk; 0 disables\n",
    "\n",
    "# Post-selection (makes it \"production\")\n",
    "MAX_KEEP = 250                 # final mask bank size per image\n",
    "DEDUP_IOU_THRESH = 0.90        # drop masks that overlap too much with kept ones\n",
    "\n",
    "# Outputs\n",
    "TOP_K_PNG = 25\n",
    "OVERLAY_TOP_K = 15\n",
    "SAVE_MASK_EMB = True           # per-mask SAM embedding (very useful for conditioning)\n",
    "# =========================\n",
    "\n",
    "\n",
    "def load_image_rgb(path: str) -> np.ndarray:\n",
    "    return np.array(Image.open(path).convert(\"RGB\"), dtype=np.uint8)\n",
    "\n",
    "\n",
    "def mask_stats(mask_bool: np.ndarray):\n",
    "    mask_bool = np.asarray(mask_bool, dtype=bool)\n",
    "    ys, xs = np.where(mask_bool)\n",
    "    if xs.size == 0:\n",
    "        return None\n",
    "\n",
    "    h, w = mask_bool.shape\n",
    "    x0, x1 = int(xs.min()), int(xs.max())\n",
    "    y0, y1 = int(ys.min()), int(ys.max())\n",
    "\n",
    "    area_frac = float(mask_bool.mean())\n",
    "    cx = float(xs.mean() / w)\n",
    "    cy = float(ys.mean() / h)\n",
    "\n",
    "    bw = float((x1 - x0 + 1) / w)\n",
    "    bh = float((y1 - y0 + 1) / h)\n",
    "    bbox_area_frac = float(((x1 - x0 + 1) * (y1 - y0 + 1)) / (h * w))\n",
    "\n",
    "    bbox_area_px = max(1, (x1 - x0 + 1) * (y1 - y0 + 1))\n",
    "    fill_frac = float(mask_bool.sum() / bbox_area_px)\n",
    "\n",
    "    return {\n",
    "        \"area_frac\": area_frac,\n",
    "        \"cx\": cx, \"cy\": cy,\n",
    "        \"bbox_w\": bw, \"bbox_h\": bh,\n",
    "        \"bbox_area_frac\": bbox_area_frac,\n",
    "        \"bbox_xyxy\": [x0, y0, x1, y1],\n",
    "        \"fill_frac\": fill_frac,\n",
    "    }\n",
    "\n",
    "\n",
    "def bbox_xywh_from_mask(mask_bool: np.ndarray):\n",
    "    ys, xs = np.where(mask_bool)\n",
    "    if xs.size == 0:\n",
    "        return None\n",
    "    x0, x1 = int(xs.min()), int(xs.max())\n",
    "    y0, y1 = int(ys.min()), int(ys.max())\n",
    "    return [x0, y0, int(x1 - x0 + 1), int(y1 - y0 + 1)]\n",
    "\n",
    "\n",
    "# ---------- OpenCV-free min_mask_region_area replacement ----------\n",
    "def _remove_small_islands(mask: np.ndarray, min_area: int) -> np.ndarray:\n",
    "    if min_area <= 0:\n",
    "        return mask\n",
    "    lab, n = ndi.label(mask)\n",
    "    if n == 0:\n",
    "        return mask\n",
    "    sizes = ndi.sum(mask, lab, index=np.arange(1, n + 1))\n",
    "    keep = np.zeros(n + 1, dtype=bool)\n",
    "    keep[1:] = sizes >= min_area\n",
    "    return keep[lab]\n",
    "\n",
    "\n",
    "def _fill_small_holes(mask: np.ndarray, min_area: int) -> np.ndarray:\n",
    "    if min_area <= 0:\n",
    "        return mask\n",
    "    inv = ~mask\n",
    "    lab, n = ndi.label(inv)\n",
    "    if n == 0:\n",
    "        return mask\n",
    "\n",
    "    border = np.zeros_like(inv, dtype=bool)\n",
    "    border[0, :] = border[-1, :] = True\n",
    "    border[:, 0] = border[:, -1] = True\n",
    "    border_labels = np.unique(lab[border])\n",
    "\n",
    "    hole_labels = np.setdiff1d(np.arange(1, n + 1), border_labels, assume_unique=False)\n",
    "    if hole_labels.size == 0:\n",
    "        return mask\n",
    "\n",
    "    hole_sizes = ndi.sum(inv, lab, index=hole_labels)\n",
    "    small_holes = hole_labels[hole_sizes < min_area]\n",
    "    if small_holes.size == 0:\n",
    "        return mask\n",
    "\n",
    "    filled = mask.copy()\n",
    "    for hl in small_holes:\n",
    "        filled[lab == hl] = True\n",
    "    return filled\n",
    "\n",
    "\n",
    "def filter_mask_like_sam(mask: np.ndarray, min_area: int) -> np.ndarray:\n",
    "    mask = mask.astype(bool)\n",
    "    if min_area <= 0:\n",
    "        return mask\n",
    "    mask = _fill_small_holes(mask, min_area)\n",
    "    mask = _remove_small_islands(mask, min_area)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def iou(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    inter = np.logical_and(a, b).sum()\n",
    "    if inter == 0:\n",
    "        return 0.0\n",
    "    union = np.logical_or(a, b).sum()\n",
    "    return float(inter / max(1, union))\n",
    "\n",
    "\n",
    "def overlay_contours(img_rgb: np.ndarray, masks: np.ndarray, scores: np.ndarray, out_path: str, top_k=10):\n",
    "    if masks.size == 0:\n",
    "        Image.fromarray(img_rgb).save(out_path)\n",
    "        return\n",
    "    k = int(min(top_k, masks.shape[0]))\n",
    "    order = np.argsort(-scores)[:k]\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.axis(\"off\")\n",
    "    for rank, idx in enumerate(order, start=1):\n",
    "        plt.contour(masks[idx].astype(float), levels=[0.5], linewidths=2)\n",
    "        plt.text(10, 20 * rank, f\"mask {int(idx)} score={scores[idx]:.3f}\",\n",
    "                 color=\"white\", bbox=dict(facecolor=\"black\", alpha=0.5, pad=2))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def panoptic_viz(img_rgb: np.ndarray, label_map: np.ndarray, out_path: str, alpha=0.45, seed=0):\n",
    "    H, W, _ = img_rgb.shape\n",
    "    out = img_rgb.astype(np.float32).copy()\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    ids = np.unique(label_map)\n",
    "    ids = ids[ids >= 0]\n",
    "    colors = {int(i): rng.integers(0, 255, size=(3,), dtype=np.uint8) for i in ids}\n",
    "\n",
    "    overlay = img_rgb.copy()\n",
    "    for i in ids:\n",
    "        m = (label_map == i)\n",
    "        overlay[m] = colors[int(i)]\n",
    "\n",
    "    blended = (1 - alpha) * out + alpha * overlay.astype(np.float32)\n",
    "    blended = np.clip(blended, 0, 255).astype(np.uint8)\n",
    "    Image.fromarray(blended).save(out_path)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# RUN\n",
    "# =========================\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "out_masks_dir = os.path.join(OUT_ROOT, \"masks_npz\")\n",
    "out_meta_dir  = os.path.join(OUT_ROOT, \"meta\")\n",
    "out_png_dir   = os.path.join(OUT_ROOT, \"mask_png\")\n",
    "out_viz_dir   = os.path.join(OUT_ROOT, \"viz\")\n",
    "\n",
    "for d in [out_masks_dir, out_meta_dir, out_png_dir, out_viz_dir]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "if not os.path.isfile(CHECKPOINT):\n",
    "    raise FileNotFoundError(f\"Checkpoint not found: {CHECKPOINT}\")\n",
    "\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT).to(device)\n",
    "sam.eval()\n",
    "\n",
    "# Predictor = lets us compute image encoder features once per image (for mask embeddings)\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "# IMPORTANT: keep min_mask_region_area=0 to avoid cv2 import\n",
    "mask_generator = SamAutomaticMaskGenerator(\n",
    "    model=sam,\n",
    "    points_per_side=POINTS_PER_SIDE,\n",
    "    pred_iou_thresh=PRED_IOU_THRESH,\n",
    "    stability_score_thresh=STABILITY_SCORE_THRESH,\n",
    "    box_nms_thresh=BOX_NMS_THRESH,\n",
    "    crop_n_layers=CROP_N_LAYERS,\n",
    "    crop_overlap_ratio=CROP_OVERLAP_RATIO,\n",
    "    crop_n_points_downscale_factor=CROP_N_POINTS_DOWNSCALE,\n",
    "    min_mask_region_area=0,\n",
    ")\n",
    "\n",
    "paths = sorted(glob.glob(os.path.join(IN_DIR, \"*\")))\n",
    "if MAX_IMAGES != -1:\n",
    "    paths = paths[:MAX_IMAGES]\n",
    "\n",
    "for p in tqdm(paths, desc=\"AMG-prod\"):\n",
    "    name = os.path.splitext(os.path.basename(p))[0]\n",
    "    img_rgb = load_image_rgb(p)\n",
    "    H, W, _ = img_rgb.shape\n",
    "\n",
    "    # 1) Generate proposals\n",
    "    amg = mask_generator.generate(img_rgb)\n",
    "\n",
    "    # 2) Compute image embedding once (optional but powerful)\n",
    "    if SAVE_MASK_EMB:\n",
    "        predictor.set_image(img_rgb)\n",
    "        feat = predictor.get_image_embedding()  # (1,C,hf,wf), vit_b -> usually C=256, hf=wf=64\n",
    "        hf, wf = feat.shape[-2], feat.shape[-1]\n",
    "    else:\n",
    "        feat, hf, wf = None, None, None\n",
    "\n",
    "    candidates = []\n",
    "    for orig_i, m in enumerate(amg):\n",
    "        seg = m[\"segmentation\"].astype(bool)\n",
    "        if MIN_MASK_REGION_AREA > 0:\n",
    "            seg = filter_mask_like_sam(seg, MIN_MASK_REGION_AREA)\n",
    "\n",
    "        area_px = int(seg.sum())\n",
    "        if area_px == 0:\n",
    "            continue\n",
    "\n",
    "        pred_iou = float(m.get(\"predicted_iou\", 0.0))\n",
    "        stab = float(m.get(\"stability_score\", 0.0))\n",
    "        score = pred_iou * stab\n",
    "\n",
    "        st = mask_stats(seg)\n",
    "        if st is None:\n",
    "            continue\n",
    "\n",
    "        # per-mask embedding from SAM image encoder\n",
    "        emb = None\n",
    "        if SAVE_MASK_EMB:\n",
    "            mask_t = torch.from_numpy(seg[None, None].astype(np.float32)).to(device)\n",
    "            mask_small = F.interpolate(mask_t, size=(hf, wf), mode=\"nearest\")\n",
    "            denom = mask_small.sum(dim=(2, 3)) + 1e-6\n",
    "            emb_t = (feat * mask_small).sum(dim=(2, 3)) / denom  # (1,C)\n",
    "            emb = emb_t.squeeze(0).detach().cpu().to(torch.float16).numpy()  # (C,)\n",
    "\n",
    "        candidates.append({\n",
    "            \"orig_amg_index\": int(orig_i),\n",
    "            \"seg\": seg,\n",
    "            \"score\": float(score),\n",
    "            \"predicted_iou\": pred_iou,\n",
    "            \"stability_score\": stab,\n",
    "            \"area_px\": area_px,\n",
    "            \"bbox_xywh\": bbox_xywh_from_mask(seg),\n",
    "            \"stats\": st,\n",
    "            \"emb\": emb,\n",
    "        })\n",
    "\n",
    "    # 3) Sort and deduplicate (greedy IoU)\n",
    "    candidates.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "    kept = []\n",
    "    for cand in candidates:\n",
    "        if len(kept) >= MAX_KEEP:\n",
    "            break\n",
    "        ok = True\n",
    "        for prev in kept:\n",
    "            if iou(cand[\"seg\"], prev[\"seg\"]) >= DEDUP_IOU_THRESH:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            kept.append(cand)\n",
    "\n",
    "    # 4) Build final arrays\n",
    "    N = len(kept)\n",
    "    if N == 0:\n",
    "        masks = np.zeros((0, H, W), dtype=np.bool_)\n",
    "        scores = np.zeros((0,), dtype=np.float32)\n",
    "        embs = None\n",
    "        label_map = -np.ones((H, W), dtype=np.int32)\n",
    "        meta_masks = []\n",
    "    else:\n",
    "        masks = np.stack([k[\"seg\"] for k in kept], axis=0).astype(np.bool_)\n",
    "        scores = np.asarray([k[\"score\"] for k in kept], dtype=np.float32)\n",
    "\n",
    "        if SAVE_MASK_EMB:\n",
    "            embs = np.stack([k[\"emb\"] for k in kept], axis=0)  # (N,C) float16\n",
    "        else:\n",
    "            embs = None\n",
    "\n",
    "        # Non-overlapping label map: assign pixels to best mask first\n",
    "        label_map = -np.ones((H, W), dtype=np.int32)\n",
    "        occupied = np.zeros((H, W), dtype=bool)\n",
    "        order = np.argsort(-scores)\n",
    "        for new_id in order:\n",
    "            pix = masks[new_id] & (~occupied)\n",
    "            if pix.sum() == 0:\n",
    "                continue\n",
    "            label_map[pix] = int(new_id)\n",
    "            occupied[pix] = True\n",
    "\n",
    "        # Metadata\n",
    "        meta_masks = []\n",
    "        for new_id, k in enumerate(kept):\n",
    "            st = dict(k[\"stats\"])\n",
    "            st.update({\n",
    "                \"mask_index\": int(new_id),\n",
    "                \"orig_amg_index\": int(k[\"orig_amg_index\"]),\n",
    "                \"score\": float(k[\"score\"]),\n",
    "                \"predicted_iou\": float(k[\"predicted_iou\"]),\n",
    "                \"stability_score\": float(k[\"stability_score\"]),\n",
    "                \"area_px\": int(k[\"area_px\"]),\n",
    "                \"bbox_xywh\": k[\"bbox_xywh\"],\n",
    "            })\n",
    "            meta_masks.append(st)\n",
    "\n",
    "    # 5) Save compressed masks (+ embeddings)\n",
    "    packed = np.packbits(masks.reshape(masks.shape[0], -1), axis=1) if masks.shape[0] > 0 else np.zeros((0, 0), dtype=np.uint8)\n",
    "    np.savez_compressed(\n",
    "        os.path.join(out_masks_dir, f\"{name}.npz\"),\n",
    "        packed=packed,\n",
    "        shape=np.array(masks.shape, dtype=np.int32),\n",
    "        scores=scores,\n",
    "        label_map=label_map,\n",
    "        emb=embs,\n",
    "    )\n",
    "\n",
    "    with open(os.path.join(out_meta_dir, f\"{name}.json\"), \"w\") as f:\n",
    "        json.dump({\n",
    "            \"image\": p,\n",
    "            \"device\": device,\n",
    "            \"num_masks\": int(masks.shape[0]),\n",
    "            \"amg_params\": {\n",
    "                \"points_per_side\": POINTS_PER_SIDE,\n",
    "                \"crop_n_layers\": CROP_N_LAYERS,\n",
    "                \"crop_overlap_ratio\": CROP_OVERLAP_RATIO,\n",
    "                \"crop_n_points_downscale_factor\": CROP_N_POINTS_DOWNSCALE,\n",
    "                \"pred_iou_thresh\": PRED_IOU_THRESH,\n",
    "                \"stability_score_thresh\": STABILITY_SCORE_THRESH,\n",
    "                \"box_nms_thresh\": BOX_NMS_THRESH,\n",
    "                \"min_mask_region_area_post\": MIN_MASK_REGION_AREA,\n",
    "                \"dedup_iou_thresh\": DEDUP_IOU_THRESH,\n",
    "                \"max_keep\": MAX_KEEP,\n",
    "            },\n",
    "            \"masks\": meta_masks,\n",
    "        }, f, indent=2)\n",
    "\n",
    "    # 6) PNG exports for debugging\n",
    "    if masks.shape[0] > 0:\n",
    "        order = np.argsort(-scores)[:min(TOP_K_PNG, len(scores))]\n",
    "        for rank, idx in enumerate(order, start=1):\n",
    "            msk = (masks[idx].astype(np.uint8) * 255)\n",
    "            Image.fromarray(msk).save(os.path.join(out_png_dir, f\"{name}_mask{idx:04d}_rank{rank}_score{scores[idx]:.3f}.png\"))\n",
    "\n",
    "    overlay_contours(img_rgb, masks, scores, os.path.join(out_viz_dir, f\"{name}_contours.png\"), top_k=OVERLAY_TOP_K)\n",
    "    panoptic_viz(img_rgb, label_map, os.path.join(out_viz_dir, f\"{name}_panoptic.png\"), alpha=0.45, seed=0)\n",
    "\n",
    "print(f\"[DONE] outputs at: {OUT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f680f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab3875a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 21 15:12:10 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A30                     On  |   00000000:21:00.0 Off |                    0 |\n",
      "| N/A   36C    P0             31W /  165W |   21782MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    185656      C   python                                        266MiB |\n",
      "|    0   N/A  N/A   1331534      C   ...z/conda_envs/pytorch_env/bin/python        402MiB |\n",
      "|    0   N/A  N/A   1439657      C   ...PyTorch_Env_2.0/packages/bin/python      20294MiB |\n",
      "|    0   N/A  N/A   2603535      C   python                                        290MiB |\n",
      "|    0   N/A  N/A   3690214      C   ...z/conda_envs/pytorch_env/bin/python        402MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3affbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SegmentationAwareGen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
